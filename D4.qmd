# Kolokacije in sentiment {#sec-delavnica4}

```{r}
#| echo: false
#| fig-keep: 'all'
#| out-width: "100%"
#| fig-link: "https://eflmagazine.com/teach-collocations-part-2/"
knitr::include_graphics("pictures/collocation.png")
```


## Knjižnice

```{r}
#| warning: false
#| message: false

library(readtext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(udpipe)
library(janitor)
library(scales)
library(widyr)
library(syuzhet)
library(corpustools)
library(lubridate)
library(readxl)
library(writexl)
library(scales)
library(ggwordcloud)
library(FactoMineR)
library(factoextra)
library(flextable)
library(GGally)
library(ggdendro)
library(igraph)
library(network)
library(tm)

```

## Kolokacije in mere asociacije

Tole poglavje se opira na naslednje spletno gradivo:\
Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Analyzing Co-Occurrences and Collocations in R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/coll.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

in na

še eno spletno gradivo !!!

### Preberi in združi

Podatkovne nize smo pridobili s pomočjo *YouTube Data Tools* (https://tools.digitalmethods.net/netvizz/youtube/). Orodje je sprogramiral "Bernhard Rieder \[...\] an associate professor in New Media and Digital Culture at the University of Amsterdam and a researcher with the Digital Methods Initiative." (http://thepoliticsofsystems.net/about/).

Dva video posnetka na portalu *YouTube* sta povezana z znanima slovenskima politikoma, *Zoranom Jankovićem* in *Janezom Janšo*. Gledalci oddaje 24 ur so po ogledu oddali svoje pripombe. V tretjem video posnetku (file = 2) je novinarka spraševala ljudi, kaj delajo za božič. Gledalci so po ogledu oddali svoje pripombe.

```{r}
gpath <-list.files(path = "data/youtube/",
                   pattern = "_comments.csv", 
                   full.names = TRUE)

comments <- map_dfr(gpath, read_csv, .id = "file") %>% 
  # dodelimo vsaki datoteki prepoznavno ime
  # namesto ifelse() uporabljamo case_when()
  mutate(file = case_when(
    file == "1" ~ "24ur_Jansa",
    file == "2" ~ "24ur_bozic",
    file == "3" ~ "24ur_Jankovic",
    TRUE ~ "other"
  )) %>% 
  # korrekture presledkov
  mutate(text = str_squish(text),
         text = str_replace_all(text, "\\.\\.", " ... "), 
         # razmak med piko in drugo besedo
         text = str_replace_all(text, 
           pattern = "(.+[a-z])(\\.)([A-Z].+)", 
           replacement = "\\1\\2 \\3"))

names(comments)
```

### Nezaželene besede

```{r}
stop_sl <- quanteda::stopwords(language = "sl", 
                               source = "stopwords-iso")

stop_sl <- c(quanteda::stopwords(language = "sl", 
                       source = "stopwords-iso"), 
             "\n", " ", "[\\d]+", "quot", "še", "^www.+", "^http.+", 
             "search_query")

stop_sl_tidy <- stop_sl %>% as_tibble() %>% rename(word = value)

# remove stopwords and punctuation and digits
stop_sl_collapsed <- paste0(paste0('\\b', stop_sl, '\\b', 
     collapse = "|"), '|[[:punct:]]+', "[\\d]+")

# another variant
stops <- paste0(stop_sl, collapse = "\\b|\\b")

```

```{r}
comments %>% slice_sample(n = 10)
```

V nadaljevanju bomo uporabljali programske funkcije knjižnic `quanteda` in `tidytext`. Zato bomo ustvarili vzporedna korpusa - eden bo v obliki tabele (`tidy`), drugi pa v obliki seznama besedilnih enot (`quanteda corpus`). Besedilna enota bo poved, ki je za iskanje kolokacij v besedilih primerna enota.

```{r}
library(quanteda)

# iz gornje tabele s prispevki ustvari korpus
gcorp <- corpus(comments %>% mutate(files = file), 
                      text_field = "text", 
                      docid_field = "files",
                      unique_docnames = F)

# naredi seznam besednih oblik
gtoks <- tokens(gcorp)
# odstani nezaželene besedne oblike
gtoks1 <- tokens_remove(gtoks, pattern = stop_sl, padding = TRUE)

# ustvari korpus, ki vsebuje povedi kot osnovno enoto
gcorp_sent <- corpus_reshape(gcorp, to = "sentences")
```

V prejšnjem poglavju smo že videli, da lahko tabelo z besedili pretvorimo v korpus knjižnice `quanteda`. Mogoča je tudi obratna pot, tj. da korpus knjižnice `quanteda` pretvorimo v obliko tabele, ki omogoča uporabo programskih funkcij knjižnic `tidyverse` in `tidytext` (*tidy* format):

```{r}
library(tidytext)
gcorp_tidy <- gcorp %>% tidy()
gcorp_sent_tidy <- tidy(gcorp_sent)
```

### Kontingenčne tabele (tidy)

*Kolokacije* so izrazi, ki se skupaj pojavljajo (statistično značilno) pogosteje, kot bi pričakovali po naključju. Tipičen primer kolokacije v angleščini je *Merry Christmas* ali v slovenščini *Dober dan*, ker se posamezni besedi pogosteje pojavljata skupaj, kot bi pričakovali, če bi bili besedi samo naključno nanizani.

*N-grami* so sorodni kolokacijam, saj predstavljajo besede, ki se pojavljajo skupaj (bi-grami sta dve besedi, ki se pojavljata skupaj, tri-grami so tri besede in tako naprej). Sestavljanje n-gramskih seznamov je zelo enostavno.

Za sestavljanje kontinengčnih tabel želimo najprej določiti, koliko je posameznih besed v gradivu.

```{r}
words <- gcorp_sent_tidy %>% 
  unnest_tokens(word, text, "words")

# count words and remove numbers (if still present)
word_count <- words %>% 
  count(word, sort = T, name = "Freq_word") %>% 
  filter(!str_detect(word, "[\\d]+"))

# Number of words
N <- nrow(word_count)
```

Nadaljujemo lahko kar s preštevanjem ngramov, in sicer bigramov.

```{r}
# ngram tokenize
ngrams <- gcorp_sent_tidy %>% 
  unnest_tokens(ngram, text, "ngrams", n = 2)

# count ngrams and remove numbers
ngram_count <- ngrams %>% 
  count(ngram, sort = T, name = "Freq_ngram") %>% 
  filter(!str_detect(ngram, "[\\d]+"))

# Number of ngrams
NG <- nrow(ngram_count)
```

Med zgoraj navedenimi ngrami je precej nezanimivih. Zato bomo izločili ngrame, ki vsebujejo nezaželene besede (stopwords).

Prvi (daljši) način: stolpec ngram razdelimo na dva stolpca, izločimo nezaželene besede v obeh stolpcih, potem pa preštejemo preostale ngrame.

```{r}
ngrams_sep <- ngrams %>% 
  separate(ngram, c("word1", "word2"), sep = " ")

ngrams_filtered <- ngrams_sep %>%
  filter(!word1 %in% stop_sl_tidy$word) %>%
  filter(!word2 %in% stop_sl_tidy$word) %>% 
  unite(ngram, c("word1", "word2"), remove = FALSE) %>% 
  filter(ngram != "NA_NA")

ngrams_video <- ngrams_filtered %>% 
  group_by(file) %>%
  count(ngram, sort = T) %>% 
  drop_na() %>%
  group_by(file) %>%
  mutate(pct = round(100*n/sum(n), 3))
```

Drugi (krajši) način: stolpec ngram filtriramo s seznamom nezaželenih besede, ki smo jih zgoraj sestavili v vzorec za uporabo regularnih izrazov.

```{r}
#| eval: false

ngrams_video <- ngrams %>% 
  group_by(file) %>%
  filter(!str_detect(ngram, stops)) %>%
  count(ngram, sort = T) %>% 
  drop_na() %>%
  group_by(file) %>%
  mutate(pct = round(100*n/sum(n), 3))
```

```{r}
ngrams_video %>% 
  pivot_wider(names_from = file, values_from = n,
              values_fill = 0) %>% 
  ungroup() %>% rmarkdown::paged_table()
```

```{r out.width="100%", fig.width=10, fig.height=5}
library(ggwordcloud)
p2 <- ngrams_video %>% 
  slice_head(n = 45) %>% 
  mutate(angle = 10 * sample(-2:2, n(), replace = TRUE, 
                             prob = c(1, 1, 4, 1, 1))) %>% 
  ggplot(aes(label = ngram, size = n, color = pct, angle = angle)) +
  geom_text_wordcloud(shape = "circle", rm_outside = TRUE) +
  scale_size_area(max_size = 20) +
  # scale_radius(range = c(0, 16), limits = c(0, NA)) +
  facet_wrap(~ file, scales = "free")

ggsave("pictures/youtube_sl_ngrams_per_video.png", dpi = 300, 
       width = 15, height = 5)

p2
```

```{r}
library(wordcloud2)
ngrams_video %>% 
  filter(file == "24ur_bozic") %>% 
  filter(!str_detect(ngram, "href|http[s]?|[\\d]+")) %>% 
  ungroup() %>% 
  select(-pct, -file) %>% 
  wordcloud2()

```

```{r}
library(wordcloud2)
ngrams_video %>% 
  filter(file == "24ur_Jankovic") %>% 
  filter(!str_detect(ngram, "href|http[s]?|[\\d]+")) %>% 
  ungroup() %>% 
  select(-pct, -file) %>% 
  wordcloud2()
```

```{r}
library(wordcloud2)
ngrams_video %>% 
  filter(file == "24ur_Jansa") %>% 
  filter(!str_detect(ngram, "href|http[s]?|[\\d]+")) %>% 
  ungroup() %>% 
  select(-pct, -file) %>% 
  wordcloud2()
```

Preštejmo prefiltrirane ngrame!

```{r}
ngrams_filtered_count <- ngrams_filtered %>% 
  group_by(file, word1, word2) %>% 
  count(ngram, sort = T)

ngrams_filtered_count %>% 
  rmarkdown::paged_table()
```

V kontingenčno tabelo želimo zajeti vse besede. Sestavimo dve pomožni tabeli, ki ju združimo v končno kontingenčno tabelo.

```{r}
ngram_count1 <- ngrams %>% 
  count(ngram, sort = T, name = "Freq_ngram") %>% 
  filter(!str_detect(ngram, "[\\d]+")) %>% 
  # for joining we need a 'word' column for the 1st word
  separate(ngram, into = c("word", "word2"), sep = " ", remove = F)

ngram_count2 <- ngrams %>% 
  count(ngram, sort = T, name = "Freq_ngram") %>% 
  filter(!str_detect(ngram, "[\\d]+")) %>% 
  # for joining we need a 'word' column for the 2nd word
  separate(ngram, into = c("word1", "word"), sep = " ", remove = F)

```

Pogostnost izraza (*term frequency*) pogosto *normaliziramo*; pogostnost besedne oblike delimo z dolžino besedila, torej z vsoto vseh pojavnic v besedilu: TF = Freq(term) / sum(terms in doc).

```{r}
ngram_freqs <- ngram_count1 %>% 
  # join the 1st word count
  left_join(word_count, by = "word") %>% 
  rename(word1 = word, Freq_word1 = Freq_word, word = word2) %>% 
  # join the 2nd word count
  left_join(word_count, by = "word") %>% 
  rename(word2 = word, Freq_word2 = Freq_word) %>% 
  mutate(AllFreq = sum(Freq_ngram)) %>% 
  group_by(word1) %>%
  mutate(TermFreq = sum(Freq_ngram)) %>% 
  ungroup() %>% 
  group_by(word2) %>%
  mutate(CoocFreq = sum(Freq_ngram))

ngram_freqs %>% 
  ungroup() %>% 
  # the first word is the preposition "in"
  filter(word1 == "in") %>% 
  summarise(TermFreq = sum(Freq_ngram))

ngram_freqs %>% 
  # still being grouped by word2 !
  # word1 = preposition "in"
  filter(word1 == "in") %>% 
  summarise(TermFreq = sum(Freq_ngram)) %>% 
  rmarkdown::paged_table()

```

*Kontingenčna tabela*, ki vsebuje opazovane in pričakovane vrednosti pogostnosti ngramov in izračunane mere asociacije [association measures](http://www.collocations.de/AM/).

```{r}
contingency_tab <- ngram_count1 %>% 
  # join the 1st word count
  left_join(word_count, by = "word") %>% 
  rename(word1 = word, Freq_word1 = Freq_word, word = word2) %>% 
  # join the 2nd word count
  left_join(word_count, by = "word") %>% 
  rename(word2 = word, Freq_word2 = Freq_word) %>% 
  mutate(AllFreq = sum(Freq_ngram)) %>% 
  group_by(word1) %>% 
  mutate(TermFreq = sum(Freq_ngram)) %>% 
  ungroup() %>% 
  group_by(word2) %>% 
  mutate(CoocFreq = sum(Freq_ngram)) %>% 
  # Observed bigram frequencies
  rename(O11 = Freq_ngram) %>% 
  # Observed word frequencies
  mutate(O12 = Freq_word1 - O11,
         O21 = Freq_word2 - O11,
         O22 = N - (O12 + O21)) %>% 
  # Correct ???
  mutate(O12 = TermFreq - O11,
         O21 = CoocFreq - O11,
         O22 = AllFreq - (O11 + O12 + O21)) %>% 
  ungroup() %>% 
  arrange(word1) %>% 
  # Marginal frequencies
  mutate(R1 = O11 + O12, R2 = O21 + O22, 
         C1 = O11 + O21, C2 = O12 + O22) %>% 
  # Expected frequencies
  mutate(E11 = R1*C1/N, E12 = R1*C2/N, 
         E21 = R2*C1/N, E22 = R2*C2/N) %>% 
  # Association measures: Point Estimates of association strength
  mutate(MI = log(O11/E11), 
         MI_loc = O11*log(O11/E11),
         odds_ratio = log(
           ((O11+0.5)*(O22+0.5))/((O12+0.5)*(O21+0.5))), 
         odds_ratio_disc = log((O11*O22)/(O12*O21)), 
         relative_risk = log((O11*C2)/(O12*C1)), 
         Liddell = (O11*O22 - O12*O21)/(C1*C2), 
         MS = pmin(O11/R1, O11/C1), 
         gmean = O11/sqrt(R1*C1),
         Dice = 2*O11/(R1 + C1),
         Jaccard = O11/(O11 + O12 + O21)) %>% 
  # AM: Significance testing - Likelihood measures
  mutate(poiss_lik = exp(-E11)*E11^O11/factorial(O11)) %>% 
  # AM: Significance testing - Asymptotic hypothesis tests
  mutate(z_score = (O11-E11)/sqrt(E11),
         t_score = (O11-E11)/sqrt(O11),
         chisq = as.numeric(N*(O11-E11)^2/(E11*E22)),
         log_lik = 2*(O11*log(O11/E11) + O12*log(O12/E12)))

# write_csv2(contingency_tab, "data/gendering_contingency_tab1.csv")
```

```{r}
contingency_tab %>% 
  filter(str_detect(ngram, "janša")) %>% 
  filter(O11 > 1) %>% 
  arrange(-Dice) %>% 
  rmarkdown::paged_table()
```

```{r}
contingency_tab %>% 
  filter(str_detect(ngram, "zoran|zoki|janković")) %>% 
  filter(O11 > 1) %>% 
  arrange(-Dice) %>% 
  rmarkdown::paged_table()
```

```{r}
contingency_tab %>% 
  filter(str_detect(ngram, "božič")) %>% 
  filter(O11 > 1) %>% 
  arrange(-Dice) %>% 
  rmarkdown::paged_table()
```

Odstranimo nezaželene besede in manj pogoste bigrame.

```{r}
# stops <- paste0(stop_de, collapse = "\\b|\\b")

contingency_tab_stps <- contingency_tab %>% 
  filter(!str_detect(word1, stops)) %>% 
  filter(!str_detect(word2, stops))

# write_csv2(contingency_tab_stps, "data/gendering_contingency_tab_stps.csv")

```

Med preostali bigrami izstopajo ngrami iz imena in priimka, ki kažejo veliko *asociacijsko moč* (MI, Dice) in *statistično značilnost* (log_lik).

```{r}
contingency_tab_stps %>% 
  # observed freq greater than 5
  filter(O11 > 5) %>% 
  select(ngram, O11, MI, Dice) %>% 
  arrange(-Dice, -MI) %>% 
  rmarkdown::paged_table()

contingency_tab_stps %>% 
  # observed freq greater than 5
  filter(O11 > 5) %>% 
  select(ngram, O11, Dice, log_lik) %>% 
  arrange(-Dice, -log_lik) %>% 
  rmarkdown::paged_table()
```

[Association measures](http://www.collocations.de/AM/section5.html):\
Point estimates of *association strength*.

"The *association measures* presented in this section belong to the *degree of association group*. They are maximum-likelihood estimates for various coefficients of association strength (cf. Section 1). As such, all the measures are subject to large sampling errors, especially for low-frequency data. See Section 6 for an attempt to control random variation and avoid inflated association scores.

In Section 1 the two most common coefficients of association strength were introduced, the mu-value μ and the odds ratio θ. Church & Hanks (1990) used the maximum-likelihood estimate for the logarithm of μ as an association measure, which they interpreted (and motivated) as pointwise mutual information (MI), a concept from information theory. This *MI measure* is particularly prone to overestimate low-frequency data (where E11 is small), but it has nonetheless become a de facto standard in (mainly British) lexicography, often in combination with the t-score measure."

$$MI = log{\frac{O_{11}}{E_{11}}}$$ "In mathematical statistics, the *odds ratio* θ is often preferred to μ because of its convenient formal properties (cf. Agresti, 1990, Ch. 2). While the mu-value (and thus the MI score) has an intuitive interpretation (namely, that cooccurrences are μ times more likely than by chance), it is difficult to interpret θ (and thus the odds-ratio measure) in such meaningful terms."

"The best-known coefficient from this group is the *Dice coefficient*, (aka."mutual expectation") whose link function is the harmonic mean (Weisstein, 1999, s.v. Harmonic Mean)."

$$Dice = \frac{2*O_{11}}{R_{1} + C_{1}}$$

"Both the Dice coefficient and the similar Jaccard coefficient have found wide-spread use in the field of information retrieval. The Dice and Jaccard measures are fully equivalent ..."

$$Jaccard = \frac{O_{11}}{O_{11} + O_{12} + O_{21}}$$

[Asymptotic hypothesis tests](http://www.collocations.de/AM/section4.html):\
"*Asymptotic hypothesis tests* address two problems of the exact tests: (i) their numerical complexity and (ii) the difficulty of defining more"extreme" outcomes. Instead of the exact p-value, they use a much simpler equation called a test statistic. ... Association measures based on asymptotic hypothesis tests often use the test statistic itself as an association score because of its convenience. When converted into a p-value, the score can directly be compared with those of the exact hypothesis tests. ... Note that the logarithm is undefined when there are empty cells (with Oij = 0). For such cells, the entire term evaluates to zero (because 0 \* log(0) = 0 by continuous extension) and can simply be omitted from the summation."

$$log-likelihood = 2*\sum_{ij}{O_{ij} log{\frac{O_{ij}}{E_{ij}}}}$$

"Since the log-likelihood statistic has an asymptotic χ2 (chi-squared) distribution, its scores can directly be compared with those of the chi-squared measure. ... Numerical simulation shows that log-likelihood is much more conservative than chi-squared and gives an excellent approximation to the exact p-values of the Fisher measure. Therefore, it has generally been accepted as an accurate and convenient standard measure for the *significance of association*."

### N-Grami s knjižnico quanteda

Knjižnica `quanteda` ponuja odlične in zelo hitre funkcije za pridobivanje ngramov (bigramov, trigramov, ...).

```{r}
# extract bigrams
BiGrams <- gtoks %>% 
       tokens_remove(stopwords("de")) %>% 
       tokens_select(pattern = "^[A-Z]", 
                               valuetype = "regex",
                     # distinguish capital and small letters
                               case_insensitive = FALSE, 
                               padding = TRUE) %>% 
       textstat_collocations(min_count = 2, tolower = FALSE)

head(BiGrams, 20) %>% rmarkdown::paged_table()
```

```{r}
BiGrams %>% 
  slice_max(order_by = z, n = 15) %>% 
  ungroup() %>% 
  mutate(collocation = fct_reorder(collocation, z)) %>% 
  ggplot(aes(z, collocation, fill = collocation)) +
  geom_col(show.legend = F)

```

Bigrame lahko tudi zelo enostavno ekstrahiramo tudi s funkcijo `tokens_compound()`, ki privzeto sestavi dvobesedne izraze.

```{r}
ngram_extract <- tokens_compound(gtoks1, pattern = BiGrams)
```

V naslednjem koraku lahko ustvarimo *konkordance* (kwic - keywords in context).

```{r}
ngram_kwic <- kwic(ngram_extract, 
                   pattern = c("Janez_Janša", 
                               "Zoran_Janković", 
                               "Edi Pucer")) %>%
  as.data.frame() %>%
  select(-to, -from, -pattern)

library(flextable)
ngram_kwic %>% 
    flextable() %>%
  flextable::set_table_properties(width = 0.99, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "")  %>%
  flextable::border_outer()
```

Strogo gledano smo pridobili samo *n-grame*, ne pa tudi kolokacij, saj za *kolokacije* ni nujno, da se sestavni deli stikajo. V naslednjem odseku želimo v jezikovnem gradivu namesto ngramov prepoznati kolokate.

### Kolokacije

V primerjavi z ngrami je bistveno težje prepoznati kolokacije, saj so lahko med sestavnimi deli kolokacije tudi druge besede.

V prvem koraku bomo prispevke gledalcev s knjižnico `quanteda` razdelili na posamezne stavke.

```{r}
a_sents <- comments %>%
  # filter(file == "24ur_Jansa") %>% 
  pull(text) %>% 
  str_squish() %>%
  corpus(.) %>%
  corpus_reshape(to = "sentences") %>% 
  unlist() %>%
  str_remove_all("- ") %>%
  str_replace_all("\\W", " ") %>%
  str_squish()

# inspect data
head(a_sents, 2)
```

V naslednjem programskem odstavku bomo ustvarili matriko, ki prikazuje, kako pogosto se je posamezna beseda v jezikovnem gradivu pojavila skupaj z drugo besedo.

```{r}
a_corpus <- corpus(a_sents)

# clean corpus
a_clean <- tokens(a_corpus, remove_punct = T, remove_symbols = T,
                  remove_numbers = T, remove_url = T) %>%
  tokens_select(pattern = stopwords("de"), selection = "remove", padding = T)

# create document feature matrix
a_dfm <- dfm(a_clean) %>% 
  # dfm_tfidf(base = 2, scheme_tf = "prop") %>%
  # dfm_weight(scheme = "prop") %>% 
  dfm_select(pattern = stopwords("de"), selection = "remove", 
             padding = T)

# https://stackoverflow.com/questions/35265748/r-tm-create-a-table-matrix-of-term-association-frequencies-and-add-values-t (Ken Benoit)
# convert to a matrix of co-occcurences rather than counts
a_dfm <- dfm_weight(a_dfm, "boolean")

# Convert to matrix
a_sdfm <- convert(a_dfm, to = "matrix")

# convert to tm::dtm
a_dtm <- convert(a_dfm, to = "tm")

library(tm)
# convert dtm into sparse matrix
a_sdtm <- Matrix::sparseMatrix(i = a_dtm$i, j = a_dtm$j, 
                           x = a_dtm$v, 
                           dims = c(a_dtm$nrow, a_dtm$ncol),
                           dimnames = dimnames(a_dtm))

# calculate co-occurrence counts (mix: quanteda + tm + Matrix)
coocurrences <- t(a_dfm) %*% a_sdtm

# or just with quanteda dfm
coocurrences <- t(a_dfm) %*% a_dfm

# convert into dgCMatrix
collocates <- as.matrix(coocurrences)
```

```{r}
library(flextable)

collocates[1:10, 1:10] %>%
  as.data.frame() %>%
  rownames_to_column("Word") %>%
  flextable() %>%
  set_table_properties(width = .5, layout = "autofit") %>%
  theme_zebra() %>%
  fontsize(size = 12) %>%
  fontsize(size = 12, part = "header") %>%
  align_text_col(align = "center") %>%
  set_caption(caption = "")  %>%
  bold(i = 1:10, j = 1, bold = TRUE, part = "body") %>%
  border_outer()

coll_df <- collocates %>% as.data.frame() %>% 
  rownames_to_column("Term")

# write_csv2(coll_df, "data/gendering_collocates_df.csv")
# writexl::write_xlsx(coll_df, "data/gendering_collocates_dff.xlsx")
```

S funkcijo `ncol()` lahko preverimo, koliko izrazov (besed ali elementov) je predstavljenih v matriki sopojavljanja (*kookurenčni matriki*). V našem jezikovnem gradivu: 5829.

S funkcijo `summary()` lahko preverimo, kako pogosto se izrazi (term) pojavljajo v jezikovnem gradivu. Tako izvemo, da je najmanjša pogostost besede v jezikovnem gradivu (Min.) enaka 1 in največja (Max.) enaka 9664.

Velika razlika med *mediano* (24,00) in *povprečjem* (59,45) kaže, da pogostnosti besede niso normalno porazdeljene. To je za jezikovna gradiva povsem običajno.

```{r}
# inspect size of matrix
ncol(collocates)
summary(rowSums(collocates))
```

### Upodobitev kolokacij

Sedaj bomo na primeru ene posamezne besede (*janez*) pokazali, kako se izračuna *moč kolokacije za posamezne izraze* in kako jo lahko slikovno predstavljamo. Funkcija `calculateCoocStatistics` prevzemamo iz [Schweinberger 2022](https://slcladal.github.io/rscripts/calculateCoocStatistics.R) [glejte tudi @WN17].

```{r}
# load function for co-occurrence calculation
## source("https://slcladal.github.io/rscripts/calculateCoocStatistics.R")
source("rscripts/calculateCoocStatistics.R")

# define term
coocTerm <- "komunizmu" 

# calculate co-occurrence statistics
## using conversion from quanteda dfm to tm
coocs <- calculateCoocStatistics(coocTerm, a_sdtm, measure="MI")
coocs <- calculateCoocStatistics(coocTerm, a_sdtm, measure="LOGLIK")
coocs <- calculateCoocStatistics(coocTerm, a_sdtm, measure="DICE")

## using just quanteda dfm with weight "boolean"
coocs <- calculateCoocStatistics(coocTerm, a_dfm, measure="DICE")
coocs <- calculateCoocStatistics(coocTerm, a_dfm, measure="MI")
coocs <- calculateCoocStatistics(coocTerm, a_dfm, measure="LOGLIK")

## using conversion from quanteda dfm to matrix
### this takes a lot of time !
# coocs <- calculateCoocStatistics(coocTerm, b_sdfm, measure="DICE")

# inspect results
coocs[1:20]
```

Rezultat kaže, da je beseda *smrt* najmočneje povezana z besedo *komunizmu*, kar v tem jezikovnem gradivu ne preseneča. Z ozirom na precejšnjo moč povezave bi lahko trdili, da sta besedni obliki *smrt komunizmu* (podobno kot *smrt fašizmu*) kolokata in vsaj v prispevkih določenih gledalcev oddaje, ki so napisali te pripombe, celo leksikalizirane.

#### Moč asociacije {.unnumbered}

Pripravimo tabelo z vrednostmi moči asociacije.

```{r}
coocdf <- coocs %>%
  as.data.frame() %>%
  mutate(CollStrength = coocs,
                Term = names(coocs)) %>%
  filter(CollStrength > 5)

coocdf %>%
  as_tibble() %>%
  dplyr::select(-.) %>%
  dplyr::relocate(Term, CollStrength) %>%
  head(15)
```

Preprosta upodobitev moči asociacije.

```{r}
coocdf %>% 
  filter(Term != "") %>% 
ggplot(aes(x = reorder(Term, CollStrength, mean), 
                   y = CollStrength)) +
  geom_point() +
  coord_flip() +
  theme_light() +
  labs(y = "")
```

#### Dendrogrami {.unnumbered}

Dendrogrami (drevesni diagrami) prikazujejo, katere skupne značilnosti imajo podobni elementi. Dendrogrami potemtakem označujejo skupine, saj prikazujejo elemente (besede), ki so podobni ali različni glede na njihovo asociacijsko moč.

Za uporabo te metode moramo najprej ustvariti matriko razdalje iz naše matrike sopojavljanja (kookurenčne matrike).

```{r}
coocdf <- coocdf %>% filter(Term != "")
coolocs <- c(coocdf$Term, "komunizmu")

# remove non-collocating terms
collocates_redux <- collocates[rownames(collocates) %in% coolocs, ]
collocates_redux <- collocates_redux[, colnames(collocates_redux) %in% coolocs]

# create distance matrix
distmtx <- dist(collocates_redux)

clustertexts <- hclust(    # hierarchical cluster object
  distmtx,                 # use distance matrix as data
  method="ward.D2")        # ward.D as linkage method

library(ggdendro)
ggdendrogram(clustertexts) +
  scale_y_log10() +
  ggtitle("Terms strongly collocating with *komunizmu*")
```

#### Omrežni diagrami {.unnumbered}

Omrežni grafi so zelo uporabno orodje za prikaz odnosov (ali odsotnosti odnosov) med besedami.

##### Osnovni diagrami {.unnumbered}

Mrežni diagram bomo ustvarili s funkcijo `network` iz knjižnice `network`.

```{r warning=F, message=F}
library(network)

net = network::network(collocates_redux, 
                       directed = FALSE,
                       ignore.eval = FALSE,
                       names.eval = "weights")

# vertex names
network.vertex.names(net) = rownames(collocates_redux)

# inspect object
net
```

Sledi slika omrežja.

```{r warning=F, message=F}
library(GGally)

ggnet2(net, 
       label = TRUE, 
       label.size = 4,
       color = "group",
       palette = "Set2",
       alpha = 0.5,
       size.cut = 3,
       edge.alpha = 0.5) +
  guides(color = FALSE, size = FALSE)
```

Da bo diagram še bolj informativen, bomo dodali bomo še vektor besed, ki se redko, včasih ali pogosto ujemajo z izbrano besedo *komunizmu* (osnova za kategorizacijo je dendrogram, ki je prikazal grozde).

Na podlagi teh vektorjev lahko nato spremenimo ali prilagodimo privzete vrednosti določenih atributov ali parametrov omrežnega objekta (npr. uteži, izgled črt in barve).

```{r}
# create vectors with collocation occurrences as categories
mid <- c("boj", "okupiral", "jugoslavije")
high <- c("fasizmu", "smrt", "partija")
infreq <- colnames(collocates_redux)[!colnames(collocates_redux) %in% mid & !colnames(collocates_redux) %in% high]

# add color by group
net %v% "Collocation" = ifelse(network.vertex.names(net) %in% infreq, "weak", 
                   ifelse(network.vertex.names(net) %in% mid, "medium", 
                   ifelse(network.vertex.names(net) %in% high, "strong", "other")))

# modify color
net %v% "color" = ifelse(net %v% "Collocation" == "weak", "gray60", 
                  ifelse(net %v% "Collocation" == "medium", "orange", 
                  ifelse(net %v% "Collocation" == "strong", "indianred4", "gray60")))

# rescale edge size
network::set.edge.attribute(
  net, "weights", 
  ifelse(net %e% "weights" < 1, 0.1, 
         ifelse(net %e% "weights" <= 2, .5, 1)))

# define line type
network::set.edge.attribute(
  net, "lty", 
  ifelse(net %e% "weights" <= .1, 3, 
         ifelse(net %e% "weights" <= .5, 2, 1)))
```

Informativnejša različica omrežnega diagrama.

```{r net4}
ggnet2(net, 
       color = "color", 
       label = TRUE, 
       label.size = 4,
       alpha = 0.6,
       size = "degree",
       edge.size = "weights",
       edge.lty = "lty",
       edge.alpha = 0.5) +
  guides(scale = "none")
```

##### Biplot {.unnumbered}

*Biplot* se uporabljajo za prikaz rezultatov korespondenčnih analiz. Uporabni so zlasti takrat, ko nas ne zanima en določen ključni izraz in njegove kolokacije, temveč semantična podobnost številnih izrazov. Besede kot take lahko štejemo za pomensko podobne, če imajo podoben profil sopojavnosti -- tj. da se sopojavljajo z istimi besedami.

```{r}
# perform correspondence analysis
res.ca <- CA(collocates_redux, graph = FALSE)

# plot results
fviz_ca_row(res.ca, repel = TRUE, col.row = "gray20")
```

Biplot nam pokaže, da se *komunizumu* in *smrt* vedeta kot Kolokata, saj sta na diagramu razmeroma blizu (čeprav manj kot *smrt* in *fasizmu*). Biplot nam tudi pokaže druge kolokate kot npr. *rdeči* in *levicarje*.

### Statistična značilnost kolokacij

Da bi ugotovili, katere besede se skupaj pojavljajo bistveno pogosteje, kot bi pričakovali po naključju, moramo ugotoviti, ali je njihova pogostost sopojavljanja statistično značilna. To je mogoče storiti za določene ključne izraze ali pa za vse besede v jezikovnem gradivu. V tem primeru se bomo še naprej osredotočali na ključno besedo *komunizumu*.

Za določitev, kateri izrazi se pomembno ujemajo s ključnim izrazom (*komunizmu*), uporabljamo večkratne (ali ponavljajoče se) eksaktne *Fisherjeve preizkuse*, ki zahtevajo naslednje podatke:

-   a = kolikokrat se `coocTerm` pojavlja s členom j (tj. O11)\
-   b = kolikokrat se `coocTerm` pojavlja brez izraza j (tj. O12)\
-   c = Kolikokrat se drugi izrazi pojavljajo s členom j (tj. O21)\
-   d = število izrazov, ki niso `coocTerm` ali izraz j (tj. O22)

Najprej sestavljamo tabelo z gornjimi količinami.

```{r message=FALSE, warning=FALSE}
# convert to data frame
coocdf <- as.data.frame(as.matrix(collocates))

# reduce data
diag(coocdf) <- 0
coocdf <- coocdf[which(rowSums(coocdf) > 10),]
coocdf <- coocdf[, which(colSums(coocdf) > 10)]

# extract stats
cooctb <- coocdf %>%
  dplyr::mutate(Term = rownames(coocdf)) %>%
  tidyr::gather(CoocTerm, TermCoocFreq,
                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %>%
  dplyr::mutate(Term = factor(Term),
                CoocTerm = factor(CoocTerm)) %>%
  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %>%
  dplyr::group_by(Term) %>%
  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %>%
  dplyr::ungroup(Term) %>%
  dplyr::group_by(CoocTerm) %>%
  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %>%
  dplyr::arrange(Term) %>%
  # dplyr::mutate(a = TermCoocFreq,
  #               b = TermFreq - a,
  #               c = CoocFreq - a,
  #               d = AllFreq - (a + b + c)) %>%
  dplyr::mutate(O11 = TermCoocFreq,
                O12 = TermFreq - O11,
                O21 = CoocFreq - O11, 
                O22 = AllFreq - (O11 + O12 + O21)) %>%
  dplyr::mutate(NRows = nrow(coocdf))
```

```{r echo = F, message=FALSE, warning=FALSE}
cooctb %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "")  %>%
  flextable::border_outer()
```

Sedaj izberemo ključni izraz (*komunizmu*). Če bi želeli najti vse kolokacije, ki so prisotne v jezikovnem gradivu, bi uporabili celotno gradivo in ne samo podmnožico, ki vsebuje *komunizmu*.

```{r message=FALSE, warning=FALSE}
cooctb_redux <- cooctb %>%
  dplyr::filter(Term == coocTerm)
```

Nato izračunamo, kateri izrazi se (znatno) pogosteje in redkeje uporabljajo z besedo *komunizmu*. Določene jezikovne zgradbe privlačijo ali odbijajo določene besede.

```{r message=FALSE, warning=FALSE}
coocStatz <- cooctb_redux %>%
  # Marginal frequencies
  mutate(R1 = O11 + O12, R2 = O21 + O22, 
         C1 = O11 + O21, C2 = O12 + O22) %>% 
  # Expected frequencies
  mutate(E11 = R1*C1/N, E12 = R1*C2/N, 
         E21 = R2*C1/N, E22 = R2*C2/N) %>% 
  # Association measures: Point Estimates of association strength
  mutate(MI = log(O11/E11), 
         MI_loc = O11*log(O11/E11),
         odds_ratio = log(
           ((O11+0.5)*(O22+0.5))/((O12+0.5)*(O21+0.5))), 
         odds_ratio_disc = log((O11*O22)/(O12*O21)), 
         relative_risk = log((O11*C2)/(O12*C1)), 
         Liddell = (O11*O22 - O12*O21)/(C1*C2), 
         MS = pmin(O11/R1, O11/C1), 
         gmean = O11/sqrt(R1*C1),
         Dice = 2*O11/(R1 + C1),
         Jaccard = O11/(O11 + O12 + O21)) %>% 
  # AM: Significance testing - Likelihood measures
  mutate(poiss_lik = exp(-E11)*E11^O11/factorial(O11)) %>% 
  # AM: Significance testing - Asymptotic hypothesis tests
  mutate(z_score = (O11-E11)/sqrt(E11),
         t_score = (O11-E11)/sqrt(O11),
         chisq = N*(O11-E11)^2/(E11*E22),
         log_lik = 2*(O11*log(O11/E11) + O12*log(O12/E12))) %>% 
  dplyr::rowwise() %>%
  # Significance tests for asscociations
  dplyr::mutate(p = as.vector(
    unlist(fisher.test(matrix(c(O11, O12, O21, O22), 
                              ncol = 2, byrow = T))[1]))) %>%
  dplyr::mutate(x2 = as.vector(
    unlist(chisq.test(matrix(c(O11, O12, O21, O22), 
                             ncol = 2, byrow = T))[1]))) %>%
  dplyr::mutate(phi = sqrt((x2/(O11 + O12 + O21 + O22)))) %>%
  dplyr::mutate(expected = as.vector(
    unlist(chisq.test(matrix(c(O11, O12, O21, O22), 
                             ncol = 2, 
                             byrow = T))$expected[1]))) %>%
  dplyr::mutate(Significance = dplyr::case_when(p <= .001 ~ "p<.001",
                                                p <= .01 ~ "p<.01",
                                                p <= .05 ~ "p<.05", 
                                                FALSE ~ "n.s."))

# write_csv2(coocStatz, "data/coocStatz.csv")
```

```{r echo = F, message=FALSE, warning=FALSE}
coocStatz %>%
  as.data.frame() %>%
  slice_min(order_by = p, n = 15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "")  %>%
  flextable::border_outer()
```

Zdaj v tabelo dodamo stolpce in odstranimo odvečne stolpce, tako da je tabelo lažje razumeti.

```{r message=FALSE, warning=FALSE}
coocStatz <- coocStatz %>%
  dplyr::ungroup() %>%
  dplyr::arrange(p) %>%
  dplyr::mutate(j = 1:n()) %>%
  # perform benjamini-hochberg correction
  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %>%
  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %>%
  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %>%
  # calculate corrected significance status
  dplyr::mutate(
    CorrSignificance = dplyr::case_when(
      p <= corr001 ~ "p<.001", 
      p <= corr01 ~ "p<.01", 
      p <= corr05 ~ "p<.05", FALSE ~ "n.s.")) %>%
  dplyr::mutate(p = round(p, 6)) %>%
  dplyr::mutate(x2 = round(x2, 1)) %>%
  dplyr::mutate(phi = round(phi, 2)) %>%
  dplyr::arrange(p) %>%
  dplyr::select(-O11, -O12, -O21, -O22, -j, -NRows, -corr05, -corr01, -corr001, -c(R1:E22)) %>%
  dplyr::mutate(Type = ifelse(expected > TermCoocFreq, "Antitype", "Type"))
```

```{r echo = F, message=FALSE, warning=FALSE}
coocStatz %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "")  %>%
  flextable::border_outer()
```

Rezultati kažejo, da se beseda *komunizmu* v prispevkih gledalcev močno ujema z besedami *smrt, nacijonalsocijalizmu* in *fasizmu*. Popravljene vrednosti p kažejo, da so to po Benjamini-Hochbergovem popravku za večkratno/ponavljajoče se testiranje [glejte @field2012discovering] edine pomembne kolokate besede *komunizmu* v prispevkih gledalcev oddaje. Popravki so potrebni pri izvajanju več testov, ker bi bila sicer zanesljivost rezultata testa močno zmanjšana, saj ponavljajoče testiranje povzroči znatno inflacijo $\alpha$-napake. Benjamini-Hochbergov popravek ima prednost pred bolj priljubljenim Bonferronijevim popravkom, ker je manj konzervativen in je zato manj verjetno, da bo povzročil $\beta$-napake [glej znova @field2012discovering].

## Sentimentna analiza

Sentimentno analizo (tj. analiza razpoloženja piscev v besedilih o neki tematiki) je mogoče opraviti na več načinov. Najosnovnejši pristop za sentimentno analizo je uporaba posebnega slovarja, ki dodeli slovarskim enotam vrednost na čustveni lestvici, npr. ali ima neka beseda pozitiven ali negativen čustveni naboj, ali ima neka beseda visoko, nevtralno ali nizko vrednost na čustveni lestvici, ali izraža besedo določeno čustveno kategorijo (npr. veselje, žalost, jezo, ...).

Sentimentni slovarji so najbolj učinkoviti, če je besedišče v besedilih bolj predvidljivo, tako da lahko pripravimo ustrezen sentimentni slovar (npr. za prispevke na Twitterju o določenih političnih temah), bodisi z ocenami preizkusnih oseb ali s prevodom slovarja in sentimentnih vrednosti iz drugega jezika (npr. angleščine).

V oblikoslovno revnih jezikih (npr. angleščini) je uporaba sentimentnega slovarja po navadi preprostejša in uspešnejša kot v oblikoslovno bogatih jezikih (npr. v nemščini ali slovenščini).

Knjižnica s številnimi možnostmi za analizo razpoloženja (sentimenta) (v različnih jezikih) je `syuzhet`. Tudi `udpipe` ponuja dodatne možnosti za sentimentno analizo, in sicer z negacijsko inverzijo.

### Slovarske enote

V sentimentnem slovarju za slovenščino, ki ga bomo spodaj izbrali in pripravili, so besede navedene v osnovni obliki, v besedilih pa nastopajo v različnih oblikah (različnice). Zato bomo izbrali podatkovni niz, ki vsebuje lematizirane besede.

```{r}
library(udpipe)
destfile = "slovenian-ssj-ud-2.5-191206.udpipe"

if(!file.exists(destfile)){
  jezikovni_model <- udpipe_download_model(language = "slovenian")
  udmodel_sl <- udpipe_load_model(jezikovni_model$file_model)
} else {
  file_model = destfile
  udmodel_sl <- udpipe_load_model(file_model)
}

prispevki <- comments %>% 
  mutate(text = str_replace_all(
    text, "(\\,)([a-zA-ZčšžČŠŽ].*)", "\\1 \\2"))
  
jansa <- prispevki %>% filter(file == "24ur_Jansa")
x = udpipe_annotate(udmodel_sl, x = jansa$text, 
                    doc_id = as.character(jansa$file), trace = F)
jansa_ud = as.data.frame(x)

bozic <- prispevki %>% filter(file == "24ur_bozic")
x = udpipe_annotate(udmodel_sl, x = bozic$text, 
                    doc_id = as.character(bozic$file), trace = F)
bozic_ud = as.data.frame(x)

janko <- prispevki %>% filter(file == "24ur_Jankovic")
x = udpipe_annotate(udmodel_sl, x = janko$text, 
                    doc_id = as.character(janko$file), trace = F)
janko_ud = as.data.frame(x)

udpiped <- bind_rows(jansa_ud, bozic_ud, janko_ud)
dim(udpiped)
```

Iz slovarskih enot (lem) bomo sestavili dodaten stolpec, ki bo vseboval povedi s slovarskimi enotami.

```{r}
df <- udpiped %>% 
  group_by(doc_id, paragraph_id, sentence_id) %>% 
  mutate(text = paste(lemma, collapse = " ")) %>% 
  select(doc_id, paragraph_id, sentence_id, text) %>% 
  distinct() %>% 
  # obdrži le vrstice z besedami
  filter(str_detect(text, "^[a-zA-ZčšžČŠŽ].*"))
df %>% head(5)

```

### Polariteta

S knjižnico `syuzhet` izberemo eno ali več besedil in jih s funkcijo `get_sentences()` pretvorimo v povedi (upoštevana so končna ločila).

```{r message=FALSE, warning=FALSE}
library(syuzhet)
# We need to parse the text into sentences.
v_jansa <- df %>% 
  filter(doc_id == "24ur_Jansa") %>% 
  pull(text) %>% 
  get_sentences() %>% 
  str_remove(pattern = "^[:punct:]")
v_jansa <- v_jansa[v_jansa != ""]
```

Za angleščino bi lahko izbrali več sentimentnih slovarjev za primerjavo. Za slovenščino bomo v pomnilnik naložili slovarček, ki temelji na prevodu besednih oblik iz angleščine.

```{r}
senti_sl <- read_tsv("data/sentiment/nrc_lexicon_slovenian.txt")
head(senti_sl)
```

Ta slovar moramo nekoliko prilagoditi za naše potrebe, saj potrebujemo poleg stolpca z besedo le še stolpec s sentimentno vrednostjo. Torej moramo vrednosti iz stolpcev *Positive* in *Negative* pretvoriti v eno vrednost, ki jo bomo shranili v novem stolpcu z imenom *value*.

```{r}
senti_sl <- senti_sl %>% 
  mutate(Negative = -1*Negative) %>% 
  mutate(value = Positive + Negative) %>% 
  select(word, value, Positive:Trust) %>% 
  mutate(Negative = abs(Negative))

senti_sl %>% rmarkdown::paged_table()
# write_tsv(senti_sl, "nrc_lexicon_slovenian.tsv")
```

V naslednjem koraku s funkcijo `get_sentiment()` dodelimo vsaki povedi vrednost razpoloženja (npr. vsoto posameznih vrednosti za besede v povedi). Podlaga je sentimentni slovar, v katerem so besedam dodeljene čustvene vrednosti (tj. pozitivne ali negativne vrednosti).

```{r message=FALSE, warning=FALSE}
# Then we calculate a sentiment value for each sentence.
jansa_sentiment <- get_sentiment(v_jansa, method = "nrc", 
                                 lexicon = "senti_sl")
# vsota vseh vrednosti za povedi
sum(jansa_sentiment)
# povprečje vrednosti za povedi
mean(jansa_sentiment)
# nariši histogram
hist(jansa_sentiment)
```

Vrednost prispevkov o oddaji z Janezom Janšo lahko primerjamo z vrednostmi drugih prispevkov o drugih oddajah. Postopek je enako zgoraj prikazanemu. Tukaj prikazujemo različico.

```{r}
janko <- df %>% filter(doc_id == "24ur_Jankovic")
v_janko <- get_sentences(janko$text) %>% 
  str_remove(pattern = "^[:punct:]")
v_janko <- v_janko[v_janko != ""]
janko_sentiment <- get_sentiment(v_janko, method = "nrc", 
                                 lexicon = "senti_sl")
sum(janko_sentiment)
mean(janko_sentiment)
hist(janko_sentiment)
```

Poskusimo še določiti sentimentne vrednosti za prispevke gledalcev o oddaji s tematiko *božič*.

```{r}
bozic <- df %>% filter(doc_id == "24ur_bozic")
v_bozic <- get_sentences(bozic$text) %>% 
  str_remove(pattern = "^[:punct:]")
v_bozic <- v_bozic[v_bozic != ""]
bozic_sentiment <- get_sentiment(v_bozic, method = "nrc", 
                                 lexicon = "senti_sl")
sum(bozic_sentiment)
mean(bozic_sentiment)
hist(bozic_sentiment)
```

Vsota vrednosti za prispevke *24ur_bozic* je pozitivna, povprečje pa je nekoliko višje kot za prispevke *24ur_Jansa* in *24ur_Jankovic*. Poglejmo podrobnosti!

```{r}
bozic_sentiment
```

Večinoma ničle, kar lahko pomeni nevtralno besedišče, bolj verjetno pa je, da besed ni bilo v sentimentnem slovarju.

Katere povedi so bile negativno ali pozitivno ocenjene?

```{r}
which(bozic_sentiment < 0)
which(bozic_sentiment > 0)
```

Povedi 42 in 77 sta bili ocenjeni negativno (-1,-1), povedi 26, 55 in 92 pa pozitivno (1,1,1). Poglejmo povedi!

```{r}
# negativno
v_bozic[42]
v_bozic[77]
# pozitivno
v_bozic[26]
v_bozic[55]
v_bozic[92]
```

Naredimo tabelo iz vektorjev in poglejmo še preostalo gradivo! Najprej na daljši (in razumljivejši) način:

```{r}
a <- data.frame(rep("24ur_Jansa", times = length(v_jansa)))
b <- data.frame(v_jansa)
c <- data.frame(jansa_sentiment)
jansa_df <- cbind(a,b,c)
colnames(jansa_df) <- c("doc_id", "text", "value")

a <- data.frame(rep("24ur_Jankovic", times = length(v_janko)))
b <- data.frame(v_janko)
c <- data.frame(janko_sentiment)
janko_df <- cbind(a,b,c)
colnames(janko_df) <- c("doc_id", "text", "value")

a <- data.frame(rep("24ur_bozic", times = length(v_bozic)))
b <- data.frame(v_bozic)
c <- data.frame(bozic_sentiment)
bozic_df <- cbind(a,b,c)
colnames(bozic_df) <- c("doc_id", "text", "value")

youtube_24ur <- rbind(bozic_df, janko_df, jansa_df)
```

Alternativno naredimo tabelo na elegantnejši način, tj. z dvojno zanko in manj ponavljanja programskih korakov:

```{r}
youtube_24ur <- NULL

groups <- c("24ur_Jansa", "24ur_Jankovic", "24ur_bozic")
texts <- list(v_jansa, v_janko, v_bozic)
sentis <- list(jansa_sentiment, janko_sentiment, bozic_sentiment)

for(i in 1:length(groups)){
  for(j in 1:length(texts[[i]])){
    f1 <- data.frame(groups[i])
    f2 <- data.frame(texts[[i]][j])
    f3 <- data.frame(sentis[[i]][j])
    x <- cbind(f1, f2, f3)
    colnames(x) <- c("doc_id", "text", "value")
    youtube_24ur <- rbind(youtube_24ur, x)
  }
}

```

Oba načina dajeta isti rezultat.

```{r}
youtube_24ur %>% 
  select(doc_id, value, text) %>% 
  mutate(text = str_sub(text,1,40)) %>% 
  filter(value > 0 | value < 0) %>% 
  arrange(-value) %>% 
  rmarkdown::paged_table()
```

Zdaj si lahko ogledamo porazdelitev sentimentnih vrednosti in lažje vidimo, da je zelo veliko vrednosti

```{r}
youtube_24ur %>% 
  group_by(doc_id) %>% 
  count(value, sort = F) %>% 
  pivot_wider(names_from = value, values_from = n, 
              values_fill = 0, names_sort = T)

```

Slikovni prikaz (nevtralne vrednosti so izpuščene):

```{r}
youtube_24ur %>% 
  group_by(doc_id) %>% 
  count(value) %>% 
  filter(value != 0) %>% 
  ggplot(aes(value, n, fill = doc_id)) +
  geom_col(show.legend = F) +
  theme_light() +
  facet_wrap(~ doc_id) +
  labs(x = "polariteta", y = "povedi")
```

Drugi sentimentni slovarji so lahko shranjeni v obliki `json`. Odpremo jih lahko s funkcijo iz knjižnice `jsonlite`. Slovar spodaj moramo najprej preoblikovati, da bo deloval s programsko funkcijo iz knjižnice `syuzhet`.

```{r}
dict <- jsonlite::fromJSON("data/sentiment/drugi/data_dictionary_NRC_slovenian.json")

pos <- as_tibble(dict$positive) %>% 
  rename(word = value) %>% 
  mutate(value = 1)
neg <- as_tibble(dict$negative) %>% 
  rename(word = value) %>% 
  mutate(value = -1)

sl_senti <- rbind(pos, neg)
head(sl_senti)
```

Slovar tipa VAD (Valence, Arousal, Dominance) vsebuje decimalne vrednosti čustvene valence, intenzitete in dominance) za besede, ki so bile tudi v prejšnjih dveh že prikazanih slovarjih. Za delo z določeno knjižnico jih je treba prilagoditi.

```{r}
vad_sl <- read_tsv("data/sentiment/drugi/Slovenian-sl-NRC-VAD-Lexicon.txt") %>% 
  select(-Word) %>% 
  rename(word = `Slovenian-sl`) %>% 
  rename(value = Valence) %>% 
  mutate(sentiment = ifelse(value > 0.5, "positive", "negative")) %>% 
  select(word, sentiment, value)
  
```

Veliko možnosti za sentimentno analizo ponuja tudi knjižnica `udpipe` s funkcijo `txt_sentiment()`. Sentimentni slovar, ki ga želimo uporabiti s to funkcijo, mora vsebovati stolpca z imenoma *term* in *polarity*. Ukaz `rename` poskrbi za to.

```{r}
# preimenuj stolpca
polar_sl <- senti_sl %>% 
  rename(term = word) %>% 
  rename(polarity = value)

# izvedi sentimentno analizo
jansa_polar <- txt_sentiment(
  x = jansa_ud, 
  term = "lemma", 
  polarity_terms = polar_sl, 
  polarity_negators = c("ne", "ni"), 
  polarity_amplifiers = c("zelo", "izredno", "povsem"), 
  polarity_deamplifiers = c("dokaj", "sorazmerno", "nekoliko"), 
  n_before = 1, 
  n_after = 1)

# prikaz podrobnosti
jansa_polar$data %>% 
  select(doc_id, lemma, polarity, sentiment_polarity) %>% 
  rmarkdown::paged_table()

# pregled
jansa_polar$overall %>% rmarkdown::paged_table()
```

V repozitoriju *clarin.si* najdemo slovenski sentimentni slovar KSS: Kadunc, Klemen and Robnik-Šikonja, Marko, 2017, Slovene sentiment lexicon KSS 1.1, Slovenian language resource repository CLARIN.SI, ISSN 2820-4042, http://hdl.handle.net/11356/1097.

V zip datoteki je sentimentni slovar *KSS Slolex*, ki ga uporabljamo v naslednjem preizkusu. Za delo s funkcijo `txt_sentiment()` ga moramo preoblikovati.

```{r}
negpath <- list.files("data/sentiment/drugi/kss/",
           pattern = "negative_words_Slolex",
           full.names = T)
pospath <- list.files("data/sentiment/drugi/kss/",
           pattern = "positive_words_Slolex",
           full.names = T)

neg <- read_lines(negpath) %>% 
  as_tibble() %>% 
  rename(term = value) %>% 
  mutate(polarity = -1)

pos <- read_lines(pospath) %>% 
  as_tibble() %>% 
  rename(term = value) %>% 
  mutate(polarity = 1)

kss <- rbind(pos, neg)
```

V sledečem programskem odstavku ocenjujemo s sentimentnim slovarjem *kss Slolex* polariteto besed v prispevkih gledalcev o oddaji z Janezom Janšo. Ocena za prispevke *24ur_Jansa*: -94.2 (prevladujejo negativni prispevki nad pozitivnimi)

```{r}
jansa_polar <- txt_sentiment(x = jansa_ud, 
                             term = "lemma", 
                             polarity_terms = kss,
                             polarity_negators = c("ne", "ni"),
                             polarity_amplifiers = c("zelo", "izredno"),
                             polarity_deamplifiers = c("dokaj", "sorazmerno"), 
                             n_before = 1, 
                             n_after = 1)

jansa_polar$data %>% 
  select(doc_id, lemma, polarity, sentiment_polarity) %>% 
  rmarkdown::paged_table()

jansa_polar$overall %>% rmarkdown::paged_table()
```

Ustrezen postopek za prispevke o oddaji z Zoranom Jankovićem daje oceno za *24ur_Janko*: -19.6 (prevladujejo negativne čustvene vrednosti nad pozitivnimi).

```{r}
janko_polar <- txt_sentiment(x = janko_ud, 
                             term = "lemma", 
                             polarity_terms = kss,
                             polarity_negators = c("ne", "ni"),
                             polarity_amplifiers = c("zelo", "izredno"),
                             polarity_deamplifiers = c("dokaj", "sorazmerno"), 
                             n_before = 1, 
                             n_after = 1)

janko_polar$data %>% 
  select(doc_id, lemma, polarity, sentiment_polarity) %>% 
  rmarkdown::paged_table()

janko_polar$overall %>% 
  rmarkdown::paged_table()
```

Kakšne je rezultat za prispevke o oddaji z božični tematiki? Ocene za *24ur_bozic*: 9.2 (prevladujejo pozitivne vrednosti nad negativnimi).

```{r}
bozic_polar <- txt_sentiment(x = bozic_ud, 
                             term = "lemma", 
                             polarity_terms = kss,
                             polarity_negators = c("ne", "ni"),
                             polarity_amplifiers = c("zelo", "izredno"),
                             polarity_deamplifiers = c("dokaj", "sorazmerno"), 
                             n_before = 1, 
                             n_after = 1)

bozic_polar$data %>% 
  select(doc_id, lemma, polarity, sentiment_polarity) %>% 
  rmarkdown::paged_table()

bozic_polar$overall %>% rmarkdown::paged_table()
```

```{r}
bozic_polar$overall$terms_positive
```

```{r}
bozic_polar$overall$terms_negative
```

```{r}
bozic_polar$overall$terms_negation
bozic_polar$overall$terms_amplification
bozic_polar$overall$sentiment_polarity
```

Čustveno polariteto je sentimentni slovar ocenil na osnovi majhnega števila besed (48). Rezultat torej ni zanesljiv. Tabelarni in slikovni prikaz za 48 povedi:

```{r}
bozic_polar$data %>% 
  filter(!is.na(polarity)) %>% 
  rmarkdown::paged_table() %>% 
  select(sentence, polarity) %>% 
  filter(polarity == 1)

bozic_polar$data %>% 
  filter(!is.na(polarity)) %>% 
  rmarkdown::paged_table() %>% 
  select(sentence, polarity) %>% 
  filter(polarity == -1)

bozic_polar$data %>% 
  mutate(row_id = row_number()) %>% 
  filter(!is.na(polarity)) %>% 
  select(row_id, polarity) %>% 
  ggplot(aes(row_id, polarity, color = as.factor(polarity))) +
  geom_segment(aes(x = row_id, xend = row_id, 
               y = 0, yend = polarity)) +
  theme_light() +
  geom_hline(yintercept = 0) +
  theme(legend.position = "none")

```

S knjižnico `syuzhet` in funkcijo `get_sentiment` je bil rezultat skoraj enak prejšnemu, ko smo polariteto besed ocenjevali s sentimentnim slovarjem *senti_sl* (gl. zgoraj). Namesto slovarskih enot uporabljamo besedne oblike.

```{r}
kss_nrc <- kss %>% 
  rename(word = term) %>% 
  rename(value = polarity)

bozic <- prispevki %>% filter(file == "24ur_bozic")
v_bozic2 <- get_sentences(bozic$text) %>% 
  str_remove(pattern = "^[:punct:]")
v_bozic2 <- v_bozic2[v_bozic2 != ""]
bozic2_sentiment <- get_sentiment(v_bozic2, method = "nrc", 
                                 lexicon = "kss_nrc")
sum(bozic2_sentiment)
mean(bozic2_sentiment)
hist(bozic2_sentiment)

```

### Čustvene lestvice

S funkcijo `get_nrc_sentiment()` je poleg polarnih vrednosti (pozitivno vs. negativno) mogoče pridobiti oceniti čustveni naboj besed, npr. veselje (joy), žalost (sadness) in nekatera druga, spet na podlagi besednih seznamov, ki so jih ocenile preizkusne osebe.

To razmeroma dobro deluje v angleščini, španščini, nemščini in nekaterih drugih jezikih, žal rezultati v slovenščini po navadi niso uporabni. Vzemimo angleške podnapise iz filma *Avatar* za zgled in jih pripravimo za sentimentno analizo:

```{r}
avatar_eng <- read_lines("data/sub/Avatar_eng.txt")
c1 = avatar_eng %>% 
  as_tibble() %>% 
  mutate(row_tc = row_number()) %>% 
  filter(str_detect(value, "-->")) %>% 
  rename(timecode = value)

c2 = avatar_eng %>% 
  as_tibble() %>% 
  mutate(row_id = row_number()) %>% 
  filter(str_detect(value, "[a-zA-Z]")) %>% 
  rename(text = value) %>% 
  mutate(text = str_replace(text, "\\<i\\>", "")) %>% 
  mutate(text = str_replace(text, "\\</i\\>", "")) %>% 
  mutate(language = "eng")

avatxt_eng = c2 %>% 
  # opcija: dodaj stolpec s prvotnimi številkami vrstic v podnapisih
  mutate(sentence_id = row_number()) %>% 
  # regex: odstrani pomišljaj pred besedo
  mutate(text = str_replace(text, "(–)([a-zA-ZčšžČŠŽ]+)", "\\2"))

head(avatxt_eng)
```

```{r message=FALSE, warning=FALSE}
v_avatar_eng <- get_sentences(avatxt_eng$text)
rs_nrc_sentiment <- get_nrc_sentiment(v_avatar_eng, 
                                      language = "english")
joy_items <- which(rs_nrc_sentiment$joy > 0)
head(v_avatar_eng[joy_items], 4)

```

Tukaj je odlomek iz tabele, ki jo dobimo po oceni čustvenih vrednosti besed:

```{r message=FALSE, warning=FALSE}
rs_nrc_sentiment[1:10, 1:10] %>% rmarkdown::paged_table()
```

*čustvena valenca* (tj. polariteta, kako pozitivno ali negativno je ocenjen pomen besede):

```{r message=FALSE, warning=FALSE}
valence_rs <- (rs_nrc_sentiment[, 9]*-1) + rs_nrc_sentiment[, 10]
head(valence_rs)

```

Ocene čustvenega naboja besed v obliki stolpčnega diagrama:

```{r message=FALSE, warning=FALSE}
barplot(
  sort(colSums(prop.table(rs_nrc_sentiment[, 1:8]))), 
  horiz = TRUE, cex.names = 0.7, las = 1, col = 9:2,
  main = "Emotions in English Avatar subtitles", xlab="Percentage"
  )

```

Slikoni prikaz z `ggplot()`:

```{r message=FALSE, warning=FALSE}
library(scales)
rs_nrc_sentiment[,1:8] %>% 
  summarise(across(everything(), ~ mean(.))) %>% 
  pivot_longer(anger:trust, 
               names_to = "emotion", 
               values_to = "pct") %>% 
  mutate(emotion = fct_reorder(emotion, pct)) %>% 
  ggplot(aes(pct, emotion, fill = emotion)) +
  geom_col() +
  theme(legend.position = "none") +
  scale_x_continuous(labels = percent) +
  labs(x = "", y = "", 
       title = "Emotion in English Avatar Subtitles")

ggsave("pictures/rs_emotions_avatar_eng.png")  
```

Primerjava z nemškimi podnapisi iz filma *Avatar*, ki jih najprej pripravimo za uporabo:

```{r}
avatar_deu <- read_lines("data/sub/Avatar_deu.txt")
c1 = avatar_deu %>% 
  as_tibble() %>% 
  mutate(row_tc = row_number()) %>% 
  filter(str_detect(value, "-->")) %>% 
  rename(timecode = value)

c2 = avatar_deu %>% 
  as_tibble() %>% 
  mutate(row_id = row_number()) %>% 
  filter(str_detect(value, "[a-zA-Z]")) %>% 
  rename(text = value) %>% 
  mutate(text = str_replace(text, "\\<i\\>", "")) %>% 
  mutate(text = str_replace(text, "\\</i\\>", "")) %>% 
  mutate(language = "deu")

avatxt_deu = c2 %>% 
  # opcija: dodaj stolpec s prvotnimi številkami vrstic v podnapisih
  mutate(sentence_id = row_number()) %>% 
  # regex: odstrani pomišljaj pred besedo
  mutate(text = str_replace(text, "(–)([a-zA-ZčšžČŠŽ]+)", "\\2"))

head(avatxt_deu)
```

Ocene čustvenih vrednosti besed v nemških podnapisih:

```{r message=FALSE, warning=FALSE}
v_avatar_deu <- get_sentences(avatxt_deu$text)
rs_nrc_sentiment_de <- get_nrc_sentiment(v_avatar_deu, 
                                         language = "german")
joy_items_de <- which(rs_nrc_sentiment_de$joy > 0)
head(v_avatar_deu[joy_items_de], 4)
```

Slikovni prikaz čustvenih vrednosti besed v nemških podnapisih:

```{r message=FALSE, warning=FALSE}
rs_nrc_sentiment_de[,1:8] %>% 
  summarise(across(everything(), ~ mean(.))) %>% 
  pivot_longer(anger:trust, 
               names_to = "emotion", 
               values_to = "pct") %>% 
  mutate(emotion = fct_reorder(emotion, pct)) %>% 
  ggplot(aes(pct, emotion, fill = emotion)) +
  geom_col() +
  theme(legend.position = "none") +
  scale_x_continuous(labels = percent) +
  labs(x = "", y = "", 
       title = "Emotion in German Avatar Subtitles")

ggsave("pictures/rs_emotions_avatar_deu.png")  
```

Ocene čustvenih vrednosti besed v nemških podnapisih so podobne tistim v angleških podnapisih, kar sicer pričakujemo, saj naj bi bil prevod pomensko skladen z izvirnikom.

### Sentiment (animacija)

Poskusimo animirati ocene čustvenih vrednosti besed v angleških podnapisih. Izračun je lahko zamuden, zato za prikaz razpoloženja v podnapisih izberemo samo približno 100 podnapisov.

```{r message=FALSE, warning=FALSE}
library(ggpage)
library(purrr)
library(gganimate)
library(tidytext)
library(zoo)

# omejujemo se na podnapise 1 do 100
prebuild <- avatxt_eng$text[1:100] %>%
  ggpage_build() %>%
  left_join(get_sentiments("afinn"), by = "word") 

midbuild <- map_df(.x = 0:50 * 10 + 1, ~ prebuild %>% 
                     mutate(score = 
                              ifelse(is.na(value), 0, value), 
                            score_smooth = zoo::rollmean(score, .x, 0),
                            score_smooth = score_smooth / 
                              max(score_smooth), 
                            rolls = .x))

anim_pages <- midbuild %>%
  ggpage_plot(aes(fill = score_smooth)) +
  scale_fill_gradient2(low = "red", high = "blue", mid = "grey", 
                       midpoint = 0) +
  guides(fill = "none") +
  labs(
    title = "Smoothed sentiment of English Avatar subtitles, <br>rolling average of {round(frame_time)}") +
  transition_time(rolls)

anim_save("pictures/sentiment_animation.gif")
```

Animacijo sprožimo takole (rdeča barva = nizke vrednosti čustvenosti po slovarju `afinn`, modra barva = visoke vrednosti čustvenosti) :

```{r, dev='png', message=FALSE, warning=FALSE}
anim_pages
```

### Barvno označevanje

Knjižnica `corpsutools` omogoča barvno označevanje besed glede na razpoloženjske vrednosti na osnovi sentimentnega slovarja.

Najprej ustvarimo `tcorpus()`.

```{r}
library(corpustools)
bozic <- prispevki %>% filter(file == "24ur_bozic")
t = create_tcorpus(bozic$text, doc_column="doc_id")
```

Iz sentimentnega slovarja *Kss Slolex* bomo naredili `dictionary()` knjižnice `quanteda`.

```{r}
negpath <- list.files("data/sentiment/drugi/kss/",
           pattern = "negative_words_Slolex",
           full.names = T)
pospath <- list.files("data/sentiment/drugi/kss/",
           pattern = "positive_words_Slolex",
           full.names = T)

negative_words <- read_lines(negpath)
positive_words <- read_lines(pospath)

slolex_dict = dictionary(list(positive = list(positive_words), negative = list(negative_words)))

# exportJson <- jsonlite::toJSON(slolex_dict)
# write(exportJson, "data/slolex_dict.json")
```

Potem sledi sentimentna ocena korpusa.

```{r}
t$code_dictionary(slolex_dict, column = 'slolex')
t$set('sentiment', 1, subset = slolex %in% c('positive','neg_negative'))
t$set('sentiment', -1, subset = slolex %in% c('negative','neg_positive'))
```

To omogoča prikaz barvno označenega besedila v panelu `Viewer`.

```{r}
#| eval: false
browse_texts(t, scale='sentiment')
```

Barvno označeno besedilo je mogoče prikazovati tudi v spletnem brskalniku in shraniti kot `html` datoteko na disku.

```{r}
#| eval: false
browse_texts(t, scale='sentiment', 
             filename = "data/sentiment_24ur_bozic.html", 
             header = "Sentiment v prispevkih gledalcev o oddaji z božično tematiko")
```

