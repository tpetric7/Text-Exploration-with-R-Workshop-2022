# Textvergleich

## Programme installieren

Installation von Programmen (Paketen): Wenn Sie die Programme bereits installiert haben, können Sie diesen Schritt überspringen.

Das Zeichen \# im Programmblock (chunk) bedeutet, dass diese Zeile nicht ausgeführt wird. Entfernen Sie \# vor dem Installationsbefehl, falls Sie ein Programm installieren möchten.

Der folgende Programmblock automatisiert die Installation von erwünschten, aber noch nicht installierten Programmen. 

```{r message=FALSE, warning=FALSE}
# install.packages("readtext")

## First specify the packages of interest
packages = c("tidyverse", "quanteda", "quanteda.textplots", 
             "quanteda.textstats", "wordcloud2", "tidytext", 
             "udpipe", "janitor", "scales", "widyr", "syuzhet", 
             "corpustools", "readtext")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```


## Programme laden

Zuerst müssen wir die Programme ausführen, die wir für die geplante Arbeit benötigen.

```{r message=FALSE, warning=FALSE}
library(readtext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(udpipe)
library(janitor)
library(scales)
library(widyr)
library(syuzhet)
library(corpustools)

```


## Texte öffnen

```{r message=FALSE, warning=FALSE}
txt = readtext("data/books/*.txt", encoding = "UTF-8")
txt

```

Alternativ können Sie die Texte auch aus dem Internet auf Ihren Computer laden und öffnen:

```{r message=FALSE, warning=FALSE}
txt1 = readtext("https://raw.githubusercontent.com/tpetric7/tpetric7.github.io/main/data/books/prozess.txt", encoding = "UTF-8")
txt2 = readtext("https://raw.githubusercontent.com/tpetric7/tpetric7.github.io/main/data/books/tom.txt", encoding = "UTF-8")

# Datoteki združimo
txt = rbind(txt1,txt2)

```

## Korpus anlegen

Wir erstellen ein Korpus, d.h. eine Textsammlung. Der Befehl im Programmbündel `quanteda` lautet `corpus()`.

```{r message=FALSE, warning=FALSE}
romane = corpus(txt)

```

Zusammenfassung einiger grundlegender quantitativer Merkmale des sprachlichen Materials mit Hilfe von zwei `quanteda`-Funktionen: 
- `summary()`    
- `textstat_summary()`. 

```{r message=FALSE, warning=FALSE}
(romanstatistik = textstat_summary(romane)
)
```

```{r message=FALSE, warning=FALSE}
povzetek = summary(romane)
povzetek
```

Anhand der zusammengefassten Korpusdaten können Sie z.B. die durchschnittliche Satzlänge in den Texten des Korpus berechnen: 

```{r message=FALSE, warning=FALSE}
povzetek %>% 
  group_by(Text) %>%
  mutate(dolzina_povedi = Tokens/Sentences)

```

Wir könnten auch einen Indikator für die lexikalische Vielfalt in Texten berechnen, d. h. das Verhältnis zwischen Types und Tokens, was im Englischen als *type token ratio* (*ttr*) bezeichnet wird. 

Es wird unterschieden zwischen Wörterbucheinheiten (Lemmata), Wortformtypen (Types) und Wortformen (Tokens). 

So ist z. B. das deutsche Verb *gehen* eine Lexikoneinheit, die mehrere verschiedene Formen (Types) aufweist: z.B. *gehe, gehst, geht, gehen, geht, ging, gingst, ... gegangen*. 

Wortformen (Tokens): Einige Formen des Verbs (Types) kommen häufiger vor als andere, und erscheinen im ausgewählten Text nicht. 

```{r message=FALSE, warning=FALSE}
povzetek %>% 
  group_by(Text) %>% 
  mutate(ttr = Types/Tokens)

```

Das Programm `quanteda` verfügt über mehrere Optionen zur Ermittlung der lexikalischen Vielfalt, die aber eine Zerlegung der Texte in kleinere Einheiten, d. h. Tokens (Wörter, Satzzeichen usw.), erfordert. Für einige Merkmale müssen wir eine Dokumenthäufigkeitsmatrix (`dfm`, *document frequency matrix*) erstellen, in der festgehalten wird, wie oft eine Wortform in jedem Text der Textsammlung vorkommt.


## Tokenisierung

Um mehr über die Texte herauszufinden, z.B. welche Wörter in den Texten vorkommen, müssen wir zunächst eine Liste von Texteinheiten (d.h. Wörter, Satzzeichen usw.) erstellen.

Wir zerlegen die Texte in Wortformen (z. B. mit Hilfe von Leerzeichen zwischen den Wortformen als Trennungszeichen). Für die Tokenisierung gibt es in `quanteda` den Befehl `tokens()`. 

```{r message=FALSE, warning=FALSE}
besede = tokens(romane)
head(besede)

```


## Tokenliste säubern

Wir können Nicht-Wörter aus der Wortliste entfernen: 

```{r message=FALSE, warning=FALSE}
besede = tokens(romane, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T)
head(besede)

```

Wir können auch Wortformen ausschließen, die für die Inhaltsanalyse nicht erwünscht sind, sogenannte "*Stoppwörter*". Auch englische Wörter, die in den ausgewählten deutschen Texten vorkommen, können entfernt werden. Wir verketten mehrere Einheiten mit Hilfe der `c()`-Funktion (engl. *concatenate* = *verketten*). 

```{r message=FALSE, warning=FALSE}
stoplist_de = c(stopwords("de"), "dass", "Aligned", "by", "autoalignment", "Source", "Project", 
                "bilingual-texts.com", "fully", "reviewed")
besede = tokens_select(besede, pattern = stoplist_de, selection = "remove")

```

Die folgende Wortiste, mit Hilfe der `tokens()`-Funktion angelegt, wird verwendet, um eine *Konkordanz* zu erstellen, d.h. eine Liste von Kontexten, in denen ein bestimmter Suchbegriff (z.B. ein Wort oder eine Wortgruppe) vorkommt.

```{r message=FALSE, warning=FALSE}
stoplist_en = c("Aligned", "by", "autoalignment", "Source", "Project", 
                "bilingual-texts.com", "fully", "reviewed")

# Obdržali bomo ločila
woerter = tokens(romane, remove_symbols = T, remove_numbers = T, remove_url = T)
# Odstranili bomo angleške besede na začetku besedil
woerter = tokens_select(woerter, pattern = stoplist_en, selection = "remove", padding = TRUE)

```


## Kwic

Um Konkordanzen zu erstellen, verfügt das Programm `quanteda` über die `kwic()`-Funktion (*Schlüsselwort im Kontext*-Ansichten). Es ist möglich, nach einzelnen Wörtern, Phrasen und beliebigen anderen Zeichen (z.B. Wildcards wie \* ) zu suchen. 

```{r message=FALSE, warning=FALSE}
kwic(woerter, pattern = c("Frau", "Herr")) %>% head(3)

```

Wir werden die Konkordanz in eine Tabelle (oder Datensatz) umwandeln, d.h. in ein `data.frame` oder `tibble()`. Dies hat z.B. den Vorteil, dass Spaltennamen (d.h. Variablen) angegeben werden.

Die `kwic()`-Funktion hat mehrere Optionen, z.B. `case_insensitive = FALSE` unterscheidet zwischen Groß- und Kleinschreibung. Der Standardwert ist `TRUE`, d.h. dass diese Unterscheidung (wie in `Excel`) nicht getroffen wird.

```{r message=FALSE, warning=FALSE}
konkordanca = kwic(woerter, pattern = c("Frau", "Herr"), case_insensitive = FALSE) %>% 
  as_tibble()

konkordanca %>% rmarkdown::paged_table()

```

Mit dem Befehl `count()` kann man die Anzahl der im Korpus vorgefundenen Wortformen auszählen. 

```{r message=FALSE, warning=FALSE}
konkordanca %>% 
  count(keyword)

```

Im nächsten Beispiel werden Wörter mit der Endung *--in* für Substantive gesucht, die weibliche Personennamen bezeichnen (z.B. *Ärztin, Köchin, ...*). 

```{r message=FALSE, warning=FALSE}
(konkordanca2 = kwic(woerter, pattern = c("*in"), case_insensitive = FALSE) %>% 
  as_tibble()
)
```

Leider enthält die obige Liste von Kontexten viele Wortformen, die keine weiblichen Personennamen darstellen (z. B. *ein, und, ...*). Wenn wir eine genauere Liste haben wollen, müssen wir auf eine geeignetere Weise suchen, z.B. mit einer Reihe von Platzhaltern, den sogenannten *regulären Ausdrücken* (*regular expressions*, "*regex*").

Sie können reguläre Ausdrücke auf dem Portal [**https://regex101.com/**](https://regex101.com/){.uri} ausprobieren und lernen.

In dem folgenden Recherchebeispiel arbeiten wir mit regulären Ausdrücken, um möglichst wenige falsche Treffer zu erhalten.

```{r message=FALSE, warning=FALSE}
konkordanca2 = kwic(woerter, pattern = "\\A[A-Z][a-z]+[^Eae]in\\b",
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
  filter(keyword != "Immerhin", 
         keyword != "Darin",
         keyword != "Termin",
         keyword != "Worin",
         keyword != "Robin",
         keyword != "Medizin",
         keyword != "Austin",
         keyword != "Musselin",
         keyword != "Benjamin",
         keyword != "Franklin")

konkordanca2 %>% rmarkdown::paged_table()

```

Ein weiteres Beispiel für die Verwendung regulärer Ausdrücke bei der Textrecherche: Welches Diminutivsuffix ist in dem Korpus vorherrschend: *--lein* oder *--chen* ?

```{r message=FALSE, warning=FALSE}
(konkordanca3a = kwic(woerter, "*lein",
                      valuetype = "glob", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
   count(keyword, sort = TRUE)
)

(konkordanca3b <- kwic(woerter, "*chen",
                      valuetype = "glob", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
   count(keyword, sort = T)
)

(konkordanca3 <- kwic(woerter, 
                      pattern = c("\\A[A-Z][a-z]*[^aäeiouürs]chen\\b",
                                  "[A-Z]*[^kl]lein\\b"),
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
  filter(keyword != "Welchen", 
         keyword != "Manchen",
         keyword != "Solchen",
         keyword != "Fräulein")
)

```

Im folgenden Beispiel suchen wir mit Hilfe regulärer Ausdrücke das Wort *Frau* + Nachname / Vorname. Hierbei ist zwingend erforderlich, `case_insensitive = FALSE` zu setzen, da das Programm zwischen Groß- und Kleinbuchstaben unterscheiden soll.

```{r message=FALSE, warning=FALSE}
(konkordanca4 <- kwic(woerter, pattern = phrase("\\bFrau\\b ^[A-Z][^[:punct:]]"), 
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble()
)

```


## Häufigkeit

Die *Dokument-Frequenz-Matrix* (`dfm`) ist der Ausgangspunkt für die Berechnung und grafische Darstellung verschiedener statistischer Größen, z.B. auch der Häufigkeit von Wortformen in den Texten des Korpus: 

```{r message=FALSE, warning=FALSE}
matrika = dfm(besede, tolower = FALSE) # za zdaj obdržimo velike začetnice

# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)
matrika = dfm_select(matrika, selection = "remove", pattern = stoplist_de)
matrika
```

Das Programm `quanteda` verfügt über eine spezielle Funktion, die eine Liste von Wortformen und deren Häufigkeit erstellt, und zwar `textstat_frequency()`. 

```{r message=FALSE, warning=FALSE}
library(quanteda.textstats)
library(quanteda.textplots)

pogostnost = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt"))

pogostnost %>% rmarkdown::paged_table()

```

Ein Diagramm mit den gebräuchlichsten Wortformen im Korpus: 

```{r message=FALSE, warning=FALSE}
pogostnost %>% 
  slice_max(order_by = frequency, n = 20) %>% 
  mutate(feature = reorder_within(feature, frequency, frequency, sep = ": ")) %>%
  # ggplot(aes(frequency, reorder(feature, frequency))) +
  ggplot(aes(frequency, feature)) +
  geom_col(fill="steelblue") +
  labs(x = "Frequency", y = "") +
  facet_wrap(~ group, scales = "free")

```

Falls erforderlich, kann die Liste der Wortformhäufigkeiten mit der `filter()`-Funktion in zwei separate Listen aufgeteilt werden. 

```{r message=FALSE, warning=FALSE}
pogost_tom = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt")) %>% 
  filter(group == "tom.txt")

pogost_tom %>% rmarkdown::paged_table()

pogost_prozess = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt")) %>% 
  filter(group == "prozess.txt")

pogost_prozess %>% rmarkdown::paged_table()

```

*Verben des Sagens und des Denkens*: Welche kommen in den ausgewählten Texten häufiger vor? 

```{r message=FALSE, warning=FALSE}
sagen = pogostnost %>%
   filter(str_detect(feature, "^(ge)?sag*"))
sagen %>% rmarkdown::paged_table()

reden = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?rede*"))
reden %>% rmarkdown::paged_table()

fragen = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?frag*"))
fragen %>% rmarkdown::paged_table()

antworten = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?antwort*"))
antworten %>% rmarkdown::paged_table()

rufen = pogostnost %>% 
    filter(str_detect(feature, pattern = "^(ge)?ruf*", negate = FALSE)) %>% 
    filter(!str_detect(feature, "ruh|run|rum|rui|ruch"))
rufen %>% rmarkdown::paged_table()

```

```{r message=FALSE, warning=FALSE}
verb1 = sagen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "sagen")

verb2 = reden %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "reden")

verb3 = fragen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "fragen")

verb4 = antworten %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "antworten")

verb5 = rufen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "rufen")

```

Die fünf kleinen Tabellen können zu einer größeren zusammengefügt werden, z. B. mit der Funktion `rbind()` oder mit `bind_rows()`.

```{r message=FALSE, warning=FALSE}
glagoli = rbind(verb1, verb2, verb3, verb4, verb5)
glagoli %>% rmarkdown::paged_table()

```

Noch ein Diagramm: 

```{r message=FALSE, warning=FALSE}
glagoli %>% 
  ggplot(aes(freq, verb, fill = verb)) +
  geom_col() +
  facet_wrap(~ group) +
  theme(legend.position = "none")

```

Um den Vergleich von Texten zu erleichtern, kann die Tabelle auch umgestellt werden, und zwar mit der `pivot_wider()`-Funktion: 

```{r message=FALSE, warning=FALSE}
glagoli %>% 
  pivot_wider(id_cols = verb, names_from = group, values_from = freq) %>% rmarkdown::paged_table()

```

## Kollokationen

Kollexeme sind Lexikoneinheiten, die zusammen verwendet werden. Kollokationen sind sprachliche Elemente, die gemeinsam vorkommen.

*Statistische Definition*: Wenn zwei Ausdrücke (z. B. "Guten Tag") im Vergleich zu ihren unmittelbaren Nachbarn signifikant häufiger vorkommen, als man nach dem Zufall erwarten könnte, dann können sie als *Kollokation* betrachtet werden.

*Linguistische Definition*: Eine *Kollokation* ist eine semantisch verwandte Folge von Wörtern.

In `quanteda` steht uns die Funktion `textstat_collocations()` zur Auffindung von Kollokationen (im statistischen Sinne) zur Verfügung. "*woerter*" ist die Liste der Wortformen (`padding = TRUE` ist hier notwendigerweise zu setzen!), die wir oben erstellt haben. 

Kollokationen mit zwei Gliedern: 

```{r message=FALSE, warning=FALSE}
coll_2 = textstat_collocations(woerter, size = 2, tolower = TRUE) # naredi male črke !

coll_2 %>% rmarkdown::paged_table()

```

Dreigliedrige Kollokationen: 

```{r message=FALSE, warning=FALSE}
coll_3 = textstat_collocations(woerter, size = 3, tolower = FALSE)

coll_3 %>% rmarkdown::paged_table()

```

Viergliedrige Kollokationen: 

```{r message=FALSE, warning=FALSE}
coll_4 = textstat_collocations(woerter, size = 4, tolower = FALSE)

coll_4 %>% rmarkdown::paged_table()

```

Mit welchen Wortformen kommen die synonymen Fragewörter *warum* und *wieso* in unserem Korpus häufiger gemeinsam vor? 

```{r message=FALSE, warning=FALSE}
warum <- coll_2 %>% 
  filter(str_detect(collocation, "^warum"))
warum %>% rmarkdown::paged_table()

wieso <- coll_2 %>% 
  filter(str_detect(collocation, "^wieso"))
wieso %>% rmarkdown::paged_table()

```

Kollokation von Nominalphrasen (NP). Im Deutschen werden substantivische Wörter groß geschrieben. Daher erstellen wir zunächst eine Liste der großgeschriebenen Wortformen im Korpus (*woerter_caps*). Daraus können wir eine Liste von Kollokationen erhalten (*coll_caps2*). 

```{r message=FALSE, warning=FALSE}
woerter_caps = tokens_select(woerter, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE, 
                                padding = TRUE)

coll_caps2 = textstat_collocations(woerter_caps, size = 2, tolower = FALSE, min_count = 5)

coll_caps2 %>% rmarkdown::paged_table()

```

Es macht keinen Sinn, die Wortverbindung "Der/Die/Das + Substantiv" als Kollokation zu betrachten, da die überwiegende Mehrheit der Substantive im Deutschen mit dem Artikel auftritt.

Deshalb werden wir den großen Anfangsbuchstaben von einigen Funktionswörtern am Satzanfang (z.B. die Artikel *Der, Die, Das* und einige andere Wortformen), in Kleinbuchstaben umwandeln. 

```{r message=FALSE, warning=FALSE}
woerter_small = 
  tokens_replace(
    woerter, 
    pattern = c("Der","Die","Das","Des", "Wollen","Im","Zum", 
                "Kein","Jeden","Wenn", "Als", "Da","Aber", 
                "Und","Sehen"), 
    replacement = c("der","die","das", "des","wollen","im", 
                    "zum", "kein","jeden", "wenn", "als","da", 
                    "aber", "und","sehen"))

woerter_caps = tokens_select(woerter_small, pattern = "^[A-Z]", 
                             valuetype = "regex", 
                             case_insensitive = FALSE, 
                             padding = TRUE)

coll_caps2 = textstat_collocations(woerter_caps, size = 2, 
                                   tolower = FALSE, min_count = 5)

coll_caps2 %>% rmarkdown::paged_table()

```

## Lemmatisierung 

Ein *Lemma* ist eine Lexikoneinheit. Für Substantive wird gewöhnlich der Nominativ Singular, für Verben der Infinitiv und für Adjektive der Positiv im Nominativ Singular als Lemmaform verwendet. 

Listen deutscher Lexikoneinheiten (Lemmata) oder anderer Sprachen kann man aus dem Internet auf die eigene Festplatte herunterladen. Im folgenden Programmblock setzen wir solch eine Lemmaliste aus dem Internet zur Lemmatisierung der Wortformen in unserem Korpus ein. Das bewerkstelligen wir mit der `quanteda`-Funktion `tokens_replace()`. 

```{r message=FALSE, warning=FALSE}
# Preberi seznam slovarskih enot in pojavnic z diska
lemdict = read.delim2("data/lemmatization_de.txt", sep = "\t", encoding = "UTF-8", 
                      col.names = c("lemma", "word"), stringsAsFactors = F)

# Pretvori podatkovna niza v znakovna niza
lemma = as.character(lemdict$lemma) 
word = as.character(lemdict$word)

# Lematiziraj pojavnice v naših besedilih
lemmas <- tokens_replace(besede,
                             pattern = word,
                             replacement = lemma,
                             case_insensitive = TRUE, 
                             valuetype = "fixed")
```

Anschließen erstellen wir eine Matrix mit Lemmas (anstelle von Wortformen). 

```{r message=FALSE, warning=FALSE}
matrika_lem = dfm(lemmas, tolower = FALSE) # za zdaj obdržimo velike začetnice

# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)
matrika_lem = dfm_select(matrika_lem, selection = "remove", pattern = stoplist_de)
matrika_lem

```

## Wortwolken

Mit der `quanteda`-Funktion `texplot_wordcloud()` erstellen wir eine einfache Wortwolke aus der zuvor gespeicherten Dokument-Häufigkeits-Matrix, in der die Wortformen durch Lemmas ersetzt wurden. 

```{r message=FALSE, warning=FALSE}
textplot_wordcloud(matrika_lem, comparison = TRUE, adjust = 0.3, color = c("darkblue","darkgreen"),
                   max_size = 4, min_size = 0.75, rotation = 0.5, min_count = 30, max_words = 250)

```

Ästhetisch ansprechendere Wortwolken bilden wir mit dem Programm `wortcloud2`. Die erste Wortwolke zeigt Wörter aus dem ersten Text, die andere für den zweiten. In beiden Fällen verwenden wir die zuvor gespeicherte Dokument-Häufigkeits-Matrix, filtern sie jedoch durch die Hinzufügung eines Wertes in eckigen Klammern: *matrika_lem[1,]* bzw. *matrika_lem[2,]*. Diese Schreibweise in `R` besagt, dass alle Spalten der Matrix verwendet werden sollen, aber nur die erste bzw. zweite Zeile. In den Spalten der Matrix stehen nämlich die Lemmas, die erste Zeile bezieht sich auf den ersten Text und die zweite auf den zweiten Text. 

```{r message=FALSE, warning=FALSE}
# install.packages("wordcloud2)
matrika_lem_prozess = matrika_lem[1,]

set.seed(1320)
library(wordcloud2)
topfeat <- as.data.frame(topfeatures(matrika_lem_prozess, 100))
topfeat <- rownames_to_column(topfeat, var = "word")
wordcloud2(topfeat)

```

```{r message=FALSE, warning=FALSE}
matrika_lem_tom = matrika_lem[2,]

set.seed(1321)
library(wordcloud2)
topfeat2 <- as.data.frame(topfeatures(matrika_lem_tom, 100))
topfeat2 <- rownames_to_column(topfeat2, var = "word")
wordcloud2(topfeat2)

```

## Position im Text (xray)

Ein xray-Diagramm zeigt die Verteilung einer Zeichenkette in einem oder mehreren Texten des Korpus an. Im folgenden Beispiel mit der `quanteda`-Funktion `textplot_xray()` sieht man schematisch, an welchen Textstellen das deutsche Wort *frau* in den Texten erscheint. Derartige Diagramme sind auch unter anderen Bezeichnungen geläufig: z.B. *Barcode*- oder *Strichcode-Diagramm*, *Dispersionsdiagramm*. Eine ähnliche graphische Funktion, genannt *MicroSearch*, begegnet uns auch in `Voyant Tools`.

```{r message=FALSE, warning=FALSE}
kwic_frau = kwic(lemmas, pattern = "frau")
textplot_xray(kwic_frau)

```

Die folgende Funktion `dispersion_plot()` haben wir selber zusammengestellt und soll wie die Python-Bibliothek `NLTK` möglichst benutzerfreundlich sein. Angegeben werden muss lediglich der Name eines beliebigen Textes und ein beliebiges Wort, dass im Text ausfindig gemacht werden soll. Unter der Haube der selbstgebastelten Funktion wird der Text in Wortformen zerlegt, eine `KWIC()`-Recherche durchgeführt und letztendlich ein *xray*-Diagramm geplottet. Vorausgesetzt werden die beiden Programme `quanteda` und `quanteda.textplots`. 

```{r message=FALSE, warning=FALSE}
dispersion_plot <- function(text, word){
  library(quanteda); library(quanteda.textplots)
  tokens({text}) %>% kwic({word}) %>% textplot_xray()
  }
```

Im folgenden Beispiel möchten wir wissen, wo in unseren beiden Texten die Phrase *die frau* vorkommt. So wie in der `kWIC()`-Funktion der `quanteda`-Bibliothek kommt die (`phrase()`-Funktion zum Einsatz, da wir nicht nach einem einzelnen Wort suchen sondern nach einer Wortverbindung.   

```{r message=FALSE, warning=FALSE}
dispersion_plot(txt$text, phrase("die frau"))
```


## Lexikalische Vielfalt

Mit Hilfe der `quanteda`-Funktion `textstat_lexdiv()`, die als Eingabe eine Dokument-Frequenz-Matrix verlangt, können verschiedene Indices für lexikalische Diversität (Wortvielfalt, Wortreichtum) berechnet werden, d.h. wie viele verschiedene Wortformen in einem Text vorkommen. Der bekannteste Quotient zur Einschätzung des Formenreichtums ist das Type-Token-Verhältnis. Da aber letztgenanntes Verhältnis von der Länge des Textes bzw. der Größe des Textkorpus abhängt, sind auch andere Indices im Einsatz, um den eben genannten Nachteil zu kompensieren. 

```{r message=FALSE, warning=FALSE}
textstat_lexdiv(matrika, measure = "all")
```


## Textähnlichkeit

Mit der quanteda-Funktion textsat_simil() kann die Ähnlichkeit von Texten berechnet werden. Dieses Verfahren ist vor allem dann interessant, wenn wir mehr als zwei Texte vergleichen wollen. Deshalb fügen wir unserem bisherigen Textkorpus noch eine Novelle von Kafka hinzu. 

```{r message=FALSE, warning=FALSE}
# odpremo datoteko
verwandl = readtext("data/books/verwandlung/verwandlung.txt", encoding = "UTF-8")
# ustvarimo nov korpus
verw_corp = corpus(verwandl)
# združimo novi korpus s prrejšnjim
romane3 = romane + verw_corp
# tokenizacija
romane3_toks = tokens(romane3)
# ustvarimo matriko (dfm)
romane3_dfm = dfm(romane3_toks)

```

Das Ergebnis der Berechnung: Kafkas Novelle *Die Verwandlung* hat etwas mehr Ähnlichkeit mit Kafkas Roman *Der Prozess* als mit Twains Roman *Tom Sawyer*.

```{r message=FALSE, warning=FALSE}
textstat_simil(romane3_dfm, method = "cosine", margin = "documents")

```

Statt die `textstat_simil()`-Funktion mit ganzen Texten zu konfrontieren, kann man auch die Ähnlichkeit von ausgewählten Wortformen (`margin = features`) berechnen lassen. 

```{r message=FALSE, warning=FALSE}
# compute some term similarities
simil1 = textstat_simil(matrika, 
                        matrika[, c("Josef", "Tom", 
                                    "Sawyer", "Huck", "Finn")], 
                         method = "cosine", margin = "features")
head(as.matrix(simil1), 10)
```

Die Unterschiedlichkeit oder Distanz von Texten kann man mit Hilfe der textstat_dist() berechnen lassen. 

```{r message=FALSE, warning=FALSE}
# plot a dendrogram after converting the object into distances
dist1 = textstat_dist(romane3_dfm, 
                      method = "euclidean", margin = "documents")
plot(hclust(as.dist(dist1)))
```


## Schlüsselwörter (keywords)

Welche Wortformen können als Schlüsselwörter für einen Text angesehen werden, d.h. als Begriffe, die für einen bestimmten Text charakteristisch sind und ihn von anderen unterscheiden? Mit der  `quanteda`-Funktion `textstat_keyness()` vergleichen wir einen Zieltext (`target`) mit einem Referenztext (`reference`). 

```{r message=FALSE, warning=FALSE}
key_tom <- textstat_keyness(matrika, target = "tom.txt")
key_tom %>% rmarkdown::paged_table()

key_prozess <- textstat_keyness(matrika, target = "prozess.txt")
key_prozess %>% rmarkdown::paged_table()

```

```{r message=FALSE, warning=FALSE}
textplot_keyness(key_tom, key_tom$n_target == 1)
textplot_keyness(key_tom, key_prozess$n_target == 1)
textplot_keyness(key_tom)
textplot_keyness(key_prozess)

```


## Lesbarkeit des Textes

Ein Lesbarkeitsindex (*readibility index*) gibt Auskunft darüber, wie schwer ein Lesetext zu verstehen ist. Es gibt eine ganze Reihe von Lesbarkeitsindices, eine ganze Palette davon macht auch `quanteda` verfügbar. Die meisten sind an die englische Sprache angepasst und haben daher für andere Sprachen eingeschränkte Aussagekraft. Grundlage für die verschiedenen Lesbarkeitsindices sind meist Größen, die sich auf die Satz- und Wortlänge beziehen. Sätze, die aus vielen Wörtern bestehen, und Wörter, die aus vielen Silben oder vielen Buchstaben bestehen, sind meist nicht so leicht und schnell zu verarbeiten wie kürzere Sätze und Wörter. 

Der *Flesch-Index* ist einer bekanntesten Lesbarkeitsindices. Es gibt sogar eine Version, die an deutschsprachige Texte angepasst ist. In unserem Textkorpus zeigt sich, dass der Roman *Der Prozess* einen etwas niedrigeren Wert (52) hat als Tom Sawyer (61). Der niedrigere Indexwert bedeutet, dass Kafkas *Prozess* - wohl wegen der im Durchschnitt etwas längeren Sätze und Wörter - schwieriger zu lesen bzw. zu verstehen ist als *Tom Sawyer*.

```{r message=FALSE, warning=FALSE}
textstat_readability(romane, measure = c("Flesch", "Flesch.Kincaid", "FOG", "FOG.PSK", "FOG.NRI"))
```


## Kookurrenz-Netzwerk (FCM)

Die Feature-Kookurrenz-Matrix (FCM) gibt Auskunft darüber, welche Wörter in in einem Text oder Textkorpus häufiger miteinander verknüpft werden. Eine FCM wird in zwei Schritten ermittelt:    
- Zunächst wird eine Liste von Ausdrücken (`pattern`, *Muster*) aus einer vorher gespeicherten Matrix (`dfm`) ausgewählt, und zwar mit der `dfm_select()`-Funktion,    
- dann wird die Feature-Kookurrenz-Matrix mit Hilfe der `fcm()`-Funktion erstellt. 

Hier folgt ein Beispiel für den zweiten Text (*Tom Sawyer*).

```{r message=FALSE, warning=FALSE}
dfm_tags <- dfm_select(
  matrika[2,], 
  pattern = (c("tom", "huck", "*joe", "becky", "tante", "witwe", 
               "polly", "sid", "mary", "thatcher", "höhle", "herz", 
               "*schule", "katze", "geld", "zaun", "piraten", 
               "schatz")))
toptag <- names(topfeatures(dfm_tags, 50))
head(toptag)
```

Die FCM kann mit Hilfe der quanteda-Funktion `textplot_network()` graphisch dargestellt werden. Als Eingabe dient eine FCM, die aber vorher gefiltert werden muss, um Übersichtlichkeit oder Interpretierbarkeit zu gewährleisten. 

```{r message=FALSE, warning=FALSE}
# Construct feature-cooccurrence matrix (fcm) of tags
fcm_tom <- fcm(matrika[2,]) # besedilo 2 je tom.txt
head(fcm_tom)
top_fcm <- fcm_select(fcm_tom, pattern = toptag)
textplot_network(top_fcm, min_freq = 0.6, edge_alpha = 0.8, edge_size = 5)

```


## Grammatische Analyse

Spezielle Programme (z.B. `spacyr` oder `udpipe`) können zur grammatikalischen Analyse und Lemmatisierung von Wortformen eingesetzt werden. Das Programm `spacyr` verlangt eine zusätzliche `Python`-Umgebung. Es ist für verbreitete europäische Sprachen wie Englisch, Französich, Deutsch und einige weitere einsetzbar, aber leider nicht für Slowenisch. Das Programm `udpipe` hat zwei Vorteile: (a) es verlangt lediglich eine `R`-Installation und (b) es ist zur Zeit für mehr als sechzig Sprachen verfügbar, neben Englisch und Deutsch auch für Slowenisch. 


### Vorbereitung

Vor der ersten Benutzung müssen wir das deutsche Sprachmodell aus dem Internet herunterladen. Im nächsten Schritt laden wir das Sprachmodell in den Arbeitsspeicher unseres Computers, und zwar mit  `udpipe_load_model()`. 

Der nachfolgende Programmblock überprüft zuerst, ob das gewünschte Modell für eine bestimmte Sprache bereits im Arbeitsverzeichnis auf der Festplatte unseres Computers gespeichert ist. Ist es noch nicht auf der Festplatte, wird das Sprachmodell heruntergeladen und anschließend in den Arbeitsspeicher geladen. Ist das Sprachmodell bereits im Arbeitsverzeichnis, wird es sofort in den Arbeitsspeicher geladen und nicht noch einmal aus dem Internet heruntergeladen. 

```{r message=FALSE, warning=FALSE}
library(udpipe)
destfile = "german-gsd-ud-2.5-191206.udpipe"

if(!file.exists(destfile)){
   sprachmodell <- udpipe_download_model(language = "german")
   udmodel_de <- udpipe_load_model(sprachmodell$file_model)
   } else {
  file_model = destfile
  udmodel_de <- udpipe_load_model(file_model)
}

```

Wenn sich das Sprachmodell bereits im Arbeitsverzeichnis Ihres Computers befindet, können Sie es auch auf folgende Weise ausführen: 

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
file_model = "german-gsd-ud-2.5-191206.udpipe"
udmodel_de <- udpipe_load_model(file_model)
```

Der nächste Programmschritt ist die Annotation der Texte, und zwar mit der Funktion `udpipe_annotate()`. Die Wortformen in den Texten werden nach nun verschiedenen grammatischen Kriterien bestimmt. 

```{r message=FALSE, warning=FALSE}
# Na začetku je readtext prebral besedila, shranili smo jih v spremenljivki "txt".
x <- udpipe_annotate(udmodel_de, x = txt$text, trace = TRUE)

# # samo prvo besedilo:
# x <- udpipe_annotate(udmodel_de, x = txt$text[1], trace = TRUE)

x <- as.data.frame(x)
```

Die Struktur des Datensatzes:

```{r message=FALSE, warning=FALSE}
str(x)
```

So sieht der Datensatz mit annotatierten Wortformen und Lemmatisierung aus: 

```{r message=FALSE, warning=FALSE}
head(x, 10) %>% rmarkdown::paged_table()

```


### Vergleich Nomen : Pronomen

Nach der automatischen, mit Überschallgeschwindigkeit durchgeführten Annotation können wir uns mit der Analyse grammatischer Kategorien von Wortformen und Lemmas beschäftigen.

Wie häufig kommen  universalen Wortklassen (upos) in den Texten vor? 

```{r message=FALSE, warning=FALSE}
(tabela = x %>% 
  group_by(doc_id) %>% 
  count(upos) %>% 
  filter(!is.na(upos),
         upos != "PUNCT")
)

tabela %>% 
  mutate(upos = reorder_within(upos, n, n, sep = ": ")) %>% 
  ggplot(aes(n, upos, fill = upos)) +
  geom_col() +
  facet_wrap(~ doc_id, scales = "free") +
  theme(legend.position = "none") +
  labs(x = "Število pojavnic", y = "")

```

Zur besseren Vergleichbarkeit fügen wir noch die entsprechenden Prozentwerte hinzu. 

```{r message=FALSE, warning=FALSE}
(delezi = tabela %>% 
  mutate(prozent = n/sum(n)) %>% 
  pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent)
)

```

Welche Anteile haben die beiden Wortklassen Nomen und Pronomen in den beiden Texten? 

```{r message=FALSE, warning=FALSE}
delezi %>% 
  filter(upos %in% c("NOUN", "PRON"))

```

Sind die Anteile der Nomina und Pronomina in beiden Texten ähnlich oder verschieden? Ein $\chi^2$-Test könnte uns eine erste Antwort auf diese Frage geben. 

```{r message=FALSE, warning=FALSE}
# za hi kvadrat test potrebujemo le drugi in tretji stolpec
nominal = delezi %>% 
  filter(upos %in% c("NOUN", "PRON")) %>% 
  dplyr::select(n_doc1, n_doc2) 

chisq.test(nominal)
```

Die beiden Texte unterscheiden sich mit statistischer Signifikanz voneinander: $\chi^2$ (1) = 147,38; p \< 0,001. Die obige Häufigkeitstabelle zeigt, dass der Anteil der Pronomina im *Prozess* vergleichsweise höher ist als im *Tom Sawyer*. Für eine verlässliche sprachwissenschaftliche Interpretation, müsste man sich genauer anschauen, welche Pronomina und welche Nomina einen starken Einfluss auf dieses Zahlenverhältnis haben. Semantisch gesehen lassen sich Pronomina nicht so eindeutig auf ein Antezedens (vorangegangenes Bezugsobjekt) beziehen wie Nomina und daher weniger zuverlässige sprachliche Mittel als Nomina, insbesondere wenn die Distanz zwischen Antezedens und Pronomen größer ist oder aufgrund von konkurrierenden Bezugsobjekten schwierig ist. Formell betrachtet sind Pronomina allerdings weniger komplex als Nomina.

Wenn wir eine Wortart mit allen anderen im Datensatz vergleichen wollen, ist die Umrechnung komplizierter, denn wie in `Excel` müssen wir    
- zuerst die Summe aller Wortarten berechnen, 
- dann die Anzahl der Pronomina bzw. Nomina von der Gesamtsumme subtrahieren,    
- und letztendlich die Differenz in der 2x2-Tabelle für den $\chi^2$-Test berücksichtigen. 

```{r message=FALSE, warning=FALSE}
(zaimki = x %>% 
  group_by(doc_id) %>% 
  count(upos) %>% 
  filter(!is.na(upos),
         upos != "PUNCT") %>% 
  mutate(vsota = sum(n),
         no_noun = vsota - n[upos == "NOUN"],
         no_pron = vsota - n[upos == "PRON"]) %>% 
  filter(upos == "PRON") %>% 
  dplyr::select(doc_id, n, no_pron) %>% 
  pivot_longer(-doc_id, 
               names_to = 'kategorija', values_to = 'vrednost') %>%
  pivot_wider(id_cols = kategorija, 
              names_from = doc_id, values_from = vrednost)
)

(samostalniki = x %>% 
  group_by(doc_id) %>% 
  count(upos) %>% 
  filter(!is.na(upos),
         upos != "PUNCT") %>% 
  mutate(vsota = sum(n),
         no_noun = vsota - n[upos == "NOUN"],
         no_pron = vsota - n[upos == "PRON"]) %>% 
  filter(upos == "NOUN") %>% 
  dplyr::select(doc_id, n, no_noun) %>% 
  pivot_longer(-doc_id, 
               names_to = 'kategorija', values_to = 'vrednost') %>%
  pivot_wider(id_cols = kategorija, 
              names_from = doc_id, values_from = vrednost)
)
```

Wir führen zwei $\chi^2$-Tests durch, und zwar:   
- einen, um die Anzahl der Pronomina mit der Anzahl anderer Wortarten zu vergleichen,    
- und einen, um die Anzahl der Nomina mit anderen Wortarten zu vergleichen. 

```{r message=FALSE, warning=FALSE}
# izločimo prvi stolpec [, -1], za hi kvadrat test potrebujemo le drugi in tretji stolpec
chisq.test(zaimki[,-1])
chisq.test(samostalniki[,-1])
```

Die beiden statistischen Tests zeigen einen statistisch signifikanten Unterschied zwischen den beiden Texten an. Allerdings ist statitische Signifikanz bei so großen Stichproben wahrscheinlicher als bei kleinen. 


### Konjunktionen im Vergleich

Als nächstes haben wir vor, die Anzahl der Sätze mit koordinierender oder subordinierender Konjunktion miteinander zu vergleichen.

Die Grundannahme ist, dass Parataxe leichter zu verstehen ist als Hypotaxe. 

*Parataxe* bedeutet, dass zwei oder mehrere Sätze in einer sprachlichen Äußerungen nebengeordnet (koordiniert, gleichrangig) sind, *Hypotaxe* dagegen, das ein oder mehrere Sätze in einer sprachlichen Äußerung einem anderen Satz untergeordnet (subordiniert) ist. 

In der folgenden Auszählung berücksichtigen wir lediglich Sätze, die mit einem *Junktor* (koordinierender Konjunktion, *CCONJ*) oder einem *Subjunktor* (subordinierender Konjunktion, *SCONJ*) eingeleitet sind. Das bedeutet, dass beispielweise *Konjunktionaladverbien*, die nebengeordnete Sätze miteinander verbinden können, oder *Relativpronomen*, die untergeordnete Sätze einleiten können, in der Auszählung nicht berücksichtigt werden.   

Die Hypothesen, die beim statistischen Test geprüft werden sollen, lauten folgendermaßen:   
- $H_0$: Das zahlenmäßige Verhältnis zwischen Junktoren und Subjunktoren in den beiden Romanen ist gleich.   
- $H_1$: Das zahlenmäßige Verhältnis zwischen Junktoren und Subjunktoren in den beiden Romanen unterscheidet sich.   

```{r message=FALSE, warning=FALSE}
(vezniki = tabela %>% 
  filter(upos %in% c("CCONJ", "SCONJ")) %>% 
  mutate(prozent = n/sum(n)) %>% 
  pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent)
)

```

Die Prozentzahlen weisen darauf hin, dass der Anteil der koordinierenden Konjunktionen im Roman *Prozess* (doc1: ca. 59%) kleiner ist als im Roman *Tom Sawyer* (doc2: ca. 72%). Das könnte bedeuten, dass im *Tom Sawyer* mehr Parataxe verwendet wird als im *Prozess*. Da es sich aber um Stichproben beider Romane handelt, müssen wir einen geeigneten statistischen Test durchführen, um den beobachteten Prozentunterschied (Häufigkeitsunterschied) zu bestätigen. 

Mit dem $\chi^2$-Test prüfen wir, ob der Häufigkeitsunterschied zwischen den beiden Romanstichproben signifikant ist, also nach statistischen Kriterien groß genug ist, um die die statistische Hypothese $H_1$ zu bestätigen oder zu verwerfen. 

```{r message=FALSE, warning=FALSE}
chisq.test(vezniki[,c(2:3)])

```

Der statistische Test bestätigt, dass der Unterschied zwischen den beiden Romanstichproben statistisch signifikant ist und dass damit die Hypothese $H_1$ angenommen werden kann.

In dem soeben durchgeführten Test haben wir nur die Anzahl der Junktoren und Subjunktoren berücksichtigt. Ändert sich das statistische Ergebnis, wenn wir in der Tabelle auch die übrigen Wortarten einbeziehen? Das soll im folgenden Programmblock überprüft werden. Zuerst müssen wir die beiden relevanten Tabellen erstellen. Die erste enthält die Häufigkeiten von Junktoren im Vergleich zu Nicht-Junktoren, die zweite dagegen die Häufigkeiten von Subjunktoren im Vergleich zu Nicht-Junktoren. 

```{r message=FALSE, warning=FALSE}
(koord = tabela %>% 
  mutate(vsota = sum(n),
         no_cconj = vsota - n[upos == "CCONJ"],
         no_sconj = vsota - n[upos == "SCONJ"]) %>% 
  filter(upos == "CCONJ") %>% 
  dplyr::select(doc_id, n, no_cconj) %>% 
  pivot_longer(-doc_id, 
               names_to = 'kategorija', values_to = 'vrednost') %>%
  pivot_wider(id_cols = kategorija, 
              names_from = doc_id, values_from = vrednost)
)

(subord = tabela %>% 
  mutate(vsota = sum(n),
         no_cconj = vsota - n[upos == "CCONJ"],
         no_sconj = vsota - n[upos == "SCONJ"]) %>% 
  filter(upos == "SCONJ") %>% 
  dplyr::select(doc_id, n, no_sconj) %>% 
  pivot_longer(-doc_id, 
               names_to = 'kategorija', values_to = 'vrednost') %>%
  pivot_wider(id_cols = kategorija, 
              names_from = doc_id, values_from = vrednost)
)

```

Beide $\chi^2$-Tests bestätigen einen statistisch signifikanten Unterschied zwischen den beiden Romanen. Die Häufigkeits- bzw. Prozentzahlen deuten darauf hin, dass im *Prozess* mehr Subjunktoren verwendet werden als im *Tom Sawyer* und weniger Junktoren. 

```{r message=FALSE, warning=FALSE}
chisq.test(koord[,-1])
chisq.test(subord[,-1])

```

Das könnte man so verstehen, dass im *Tom Sawyer* Parataxe dominanter ist als im *Prozess* und dass der erste Roman leichter zu verstehen sein könnte als der letztere. Allerdings dürfen wir an dieser Stelle nicht vergessen, dass wir lediglich Stichproben erhoben haben. Wir haben ja lediglich Sätze berücksichtigt, die Junktoren oder Subjunktoren enthalten. Uneingeleitete Sätze oder Sätze, eingeleitet durch Konjunktionaladverbien, Relativpronomina u.a., haben wir in unserer Stichprobenerhebung nicht berücksichtigt. Bei Einbezug solcher Sätze könnte sich das Ergebnis wesentlich ändern. 


### Lexikalische Einheiten

Das Program `udpipe` hat jede Wortform einem Lemma (einer Lexikoneinheit) zugeordnet. Wie viele Lemmas enthalten die Texte? Zu welchen Wortarten gehören sie am häufigsten? Wir stellen die Verhältnisse tabellarisch und graphisch dar. Die allgegenwärtigen Interpunktionszeichen (Komma, Punkt usw.) werden herausgefiltert. 

```{r message=FALSE, warning=FALSE}
(tabela2 = x %>% 
  group_by(doc_id, upos) %>% 
    filter(!is.na(upos),
           upos != "PUNCT",
           upos != "X") %>% 
  distinct(lemma) %>% 
  count(lemma) %>% 
  summarise(lemmas = sum(n)) %>% 
  mutate(prozent = round(lemmas/sum(lemmas), 4)) %>% 
  arrange(-prozent)
)

tabela2 %>% 
  # slice_max(order_by = prozent, n=6) %>% 
  mutate(upos = reorder_within(upos, lemmas, paste("(",100*prozent,"%)"), sep = " ")) %>%
  ggplot(aes(prozent, upos, fill = upos)) +
  geom_col() +
  facet_wrap(~ doc_id, scales = "free") +
  theme(legend.position = "none") +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "Anteil", y = "Wortklasse")

```

Aus unseren beiden Darstellungen der Lemmahäufigkeit ist ersichtlich, dass in beiden Texten Nomina (noun) am häufigsten vertreten sind (mehr als ein Drittel aller Lemmas), gefolgt von Verben und Adjektiven. Das ist auch für andere Texte das typische Bild, da es sich bei diesen drei Klassen um *offene Wortklassen* handelt. Funktionswörter gehören zu den *geschlossenen Wortklassen*, die nur aus relativ wenigen (oft unflektierten) Einheiten bestehen und im nur sehr geringen Maße erweiterbar sind. So machen die in fast allen Texten häufig auftretenden Pronomina (Personalpronomen, demonstrativpronomen, Possessivpronomen usw.) weniger als zwei Prozent aller Lemmas in den beiden Romanen aus. Entsprechendes ist auch bei den anderen Funktionswortklassen zu beobachten: sie haben eine hohe Vorkommenshäufigkeit (Tokenfrequenz), aber geringe Lemmahäufigkeit. 


### Wortkorrelationen

Bei der Inhaltsanalyse kann es von Nutzen sein, mehr darüber zu erfahren, welche Wörter im Text miteinander häufig verknüpft werden. Oben haben wir bereits nach Kollokationen in den beiden Romanen recherchiert. Eine ähnliche Methode ist die *Korrelationsanalyse*, die sich aber nicht auf das direkte Nacheinanderauftreten von Wörtern beschränkt. Unsere nächste Untersuchungsfrage lautet daher: Welche Worthäufigkeiten nehmen parallel zu oder ab? Können wir also paarweise Korrelationen von Wörtern nachweisen? 

Zu diesem Zweck setzen wir das Programm `widyr` ein. Ein ähnliches Analysewerkzeug ist übrigens auch bei `Voyant Tools` zu finden. Im folgenden Programmchunk erstellen wir eine Tabelle, die miteinander auftretende Lemmas und ihre Korrelation anführt. 

```{r message=FALSE, warning=FALSE}
library(widyr)

# pairwise correlation
(correlations = x %>% 
  filter(dep_rel != "punct", dep_rel != "nummod") %>%
  mutate(lemma = tolower(lemma), token = tolower(token),
         lemma = str_trim(lemma), token = str_trim(token)) %>% 
  janitor::clean_names() %>%
  group_by(doc_id, lemma, token, sentence_id) %>% 
  # add_count(token) %>% 
  summarize(Freq = n()) %>% 
  arrange(-Freq) %>% 
  filter(Freq > 2) %>% 
  pairwise_cor(lemma, sentence_id, sort = TRUE) %>% 
  filter(correlation < 1 & correlation > 0.3)
)

```

Die Korrelationswerte liegen zwischen 1 und -1. Je stärker die Verbindung zwischen zwei Wortitems (Lemmas), umso höher ist der Korrelationswert. Positive Korrelatonswerte bedeuten, dass zwei Lemmas einander anziehen (d.h. häufiger miteinander auftreten), negative Korrelationswerte dagegen, dass sie einander abstoßen (d.h. seltener miteinander auftreten). 

Als erstes Beispiel wählen wir das Lemma *Zaun* aus dem Roman *Tom Sawyer*. Welche Lemmas sind positiv oder negativ damit korreliert (d.h. treten häufger miteinander auf oder umgekehrt)?

```{r message=FALSE, warning=FALSE}
correlations %>%
  filter(item1 == "zaun") %>%
  mutate(item2 = fct_reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item2)) +
  geom_col(show.legend = F) +
  coord_flip() +
  labs(title = "What tends to appear with 'Zaun'?",
       subtitle = "Among elements that appeared in at least 2 sentences")

```

Die beiden Lemmas *Spaß* und *Arbeit* sind stärker mit *Zaun* korreliert. Das macht Sinn, wenn man das entsprechende Kapitel im Roman gelesen hat: Tom Sawyer hat von seiner Tante die Aufgabe erhalten, den Gartenzaun anzustreichen. Das macht Tom überhaupt keinen Spaß. Ein Junge kommt vorbei und Tom kommt eine Idee. Er tut so, als würde die Arbeit Spaß machen. Das war nur ein Trick von Tom. Tom gelang es, den anderen Jungen zu überzeugen, dass Zaunanstreichen Spaß macht. Der andere Junge übernahm Toms Arbeit.  

Ein weiteres Beispiel aus Kafkas Roman *Der Prozess* wäre das Lemma *Gericht*.

```{r message=FALSE, warning=FALSE}
correlations %>%
  filter(item1 == "gericht") %>%
  mutate(item2 = fct_reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item2)) +
  geom_col(show.legend = F) +
  coord_flip() +
  labs(title = "What tends to appear with 'Gericht'?",
       subtitle = "Among elements that appeared in at least 2 sentences")

```

Im Diagramm sind mehrere Lemmas zu sehen, die mittelstark mit dem Lemma *Gericht* korrelieren. Am meisten Sinn ergeben in diesem Zusammenhang die Lemmas *Anklage*, *unschuldig*, *frei* und *zwingen*. 


## Sentiment

Stopnjo čustvenosti ali emocionalnosti besedila je mogoče določiti s
sentimentnim slovarjem.


### Version 1

Zuerst soll das *nrc*-Sentimentlexikon für Deutsch zum Einsatz kommen, das im Programmpaket von `syuzhet` enthalten ist. Für die Sentimentanalyse mit `syuzhet` wird ein Text erst einmal mit der Funktion `get_sentences()` in Sätze zerlegt. 

```{r message=FALSE, warning=FALSE}
library(syuzhet)

tom_v = get_sentences(txt$text[2]) # izberemo drugo besedilo: tom.txt
tom_v = (tom_v[-1]) # tako lahko izločimo prvo vrstico (uredniško pripombo)
head(tom_v[-1])

```

Die Funktion `get_sentiment()` weist den Wörtern in den Äußerungen einen *positiven* (+1), *negativen* (-1) oder *neutralen* (0) Stimmungswert (Sentiment, Emotionswert) zu. Anschließend werden diese Sentimentwerte summiert.

```{r message=FALSE, warning=FALSE}
tom_values <- get_sentiment(tom_v, 
                            method = "nrc", language = "german")
length(tom_values)
tom_values[100:110]

```

Wir binden die Äußerungen, die Emotionswerte und die Satzlänge in einen Datensatz ein. So lässt sich besser beurteilen, wie erfolgreich der Einsatz des Sentiment-Wörterbuchs in unserem Text war. Außerdem wollen wir auch einige Spalten umbenennen. 

```{r message=FALSE, warning=FALSE}
sentiment1 = cbind(tom_v, tom_values, ntoken(tom_v)) %>% 
  as.data.frame() %>% 
  rename(words = V3,
         text = tom_v,
         values = tom_values) %>% 
  mutate(doc_id = "tom.txt") %>% 
  rowid_to_column(var = "sentence")

# View(sentiment1)
sentiment1 %>% rmarkdown::paged_table()

```

Wiederholen Sie die obigen Programschritte für den anderen Text, den wir mit dem ersten vergleichen möchten, und zwar mit Kafaks *Prozess*. 

```{r message=FALSE, warning=FALSE}
prozess_v = get_sentences(txt$text[1]) # izberemo prvo besedilo: prozess.txt
prozess_v = (prozess_v[-1]) # tako lahko izločimo prvo vrstico (uredniško pripombo)
prozess_values <- get_sentiment(prozess_v, method = "nrc", language = "german")
sentiment2 = cbind(prozess_v, prozess_values, ntoken(prozess_v)) %>% 
  as.data.frame() %>% 
  rename(words = V3,
         text = prozess_v,
         values = prozess_values) %>% 
  mutate(doc_id = "prozess.txt") %>% 
  rowid_to_column(var = "sentence")

# View(sentiment2)
sentiment2 %>% rmarkdown::paged_table()

```

Durch Addition der Stimmungswerte lässt sich abschätzen, welcher Text einen größeren Anteil positiv bewerteter Wörter enthält. Zu diesem Zweck wollen wir die beiden Datensätze zusammenführen und außerdem das Format der Spalten "Wörter" und "Werte" anpassen. 

```{r message=FALSE, warning=FALSE}
sentiment = rbind(sentiment1, sentiment2) %>% as_tibble() %>% 
  mutate(values = parse_number(values),
         words = parse_number(words)) %>%
  dplyr::select(doc_id, sentence, words, values, text)

sentiment %>% rmarkdown::paged_table()

```

Das Ergebnis: Nach der obigen Methode ist der Durchschnitt der emotionalen Werte im Roman *Prozess* etwas höher (0,055) als im Roman *Tom Sawyer* (). 

Dieses Ergebnis war unerwartet, denn *Tom Sawyer* enthält viele heitere Geschichten. Aber möglicherweise dominieren bestimmte Kapitel mit negativ konnotierten Wörtern (z.B. Flüche, heutzutage als rassistisch angesehene Wörter wie z.B. *Nigger* u.a.)  und vielleicht auch die unheimlichen oder gefährlichen Begegnungen mit bestimmten Personen das Gesamtergebnis beeinflusst haben. Natürlich kann es durchaus sein, dass unser Sentimentlexikon zu viele Wortformen nicht erfasst hat und daher das Ergebnis verfälscht ist. 

```{r message=FALSE, warning=FALSE}
sentiment %>% 
  group_by(doc_id) %>% 
  summarise(polarnost = mean(values))

```

Da wir die beiden Romane nicht so schnell (erneut) durchlesen können, sollten wir es noch auf andere Weise versuchen: Behandeln wir doch positive, neutrale und negative Werte getrennt und berücksichtigen wir dabei auch die Satzlänge!

```{r message=FALSE, warning=FALSE}
sentiment1 = sentiment %>% 
  group_by(doc_id) %>% 
  mutate(positive = ifelse(values > 0, abs(values), 0),
         neutral = ifelse(values == 0, 1, 0),
         negative = ifelse(values < 0, abs(values), 0))
sentiment1 %>% 
  summarise(pos = mean(100*positive/words),
            neut = mean(100*neutral/words),
            neg = mean(100*negative/words))

```

Alle durchschnittlichen Sentimentwerte (egal, ob positiv, negativ oder neutral) sind in *Tom Sawyer* etwas höher als im *Prozess*. Am deutlichsten ist der Unterschied zwischen den als neutral bewerteten Wörtern. 

Versuchen wir mit einem `t-Test` herauszufinden, welche Unterschiede statistisch signifikant sind! Da wir die Varianzen der Sentimentwerte nicht kennen, wählen wir den *Welch*-t-Test, der bei ungleichen Varianzen zum Einsatz kommt (`var.equal = FALSE`). 

```{r message=FALSE, warning=FALSE}
t.test(positive ~ doc_id, data = sentiment1, var.equal = FALSE)
t.test(negative ~ doc_id, data = sentiment1, var.equal = FALSE)
t.test(neutral ~ doc_id, data = sentiment1, var.equal = FALSE)
```

Zwei t-Tests weisen einen signifikanten Unterschied aus, und zwar bei positiven und neutralen Sentimentwerten. Bei negativen Sentimentwerten ist der Unterschied zwischen den beiden Romanen nicht signifikant. 

Dieses Ergebnis entspricht eher unseren oben beschriebenen Erwartungen: *Tom Sawyer* scheint im Durchschnitt positivere und neutralere Sentimentwerte aufzuweisen als der *Prozess*, bei negativen Sentimentwerten konnte dagegen kein signifikanter Unterschied zwischen den beiden Romanen festgestellt werden. 

Schauen wir uns nun einige der Sätze an, die negativ bewertet wurden:

```{r message=FALSE, warning=FALSE}
sentiment1 %>% 
  filter(negative > 0) %>% 
  rmarkdown::paged_table()
```


### Variante 2

Im zweiten Versuch wählen wir das *nrc*-Sentimentlexikon, und zwar mit Hilfe der Funktion `get_nrc_sentiment()`. 

```{r message=FALSE, warning=FALSE}
tom_v = get_sentences(txt$text[2])
tom_nrc_values = get_nrc_sentiment(tom_v)
tom_joy_items = which(tom_nrc_values$joy > 0)
head(tom_v[tom_joy_items], 4)

```

```{r message=FALSE, warning=FALSE}
nrc_sentiment = as.data.frame(cbind(tom_v, tom_nrc_values))
nrc_sentiment %>% rmarkdown::paged_table()

```

Die beiden folgenden linearen Regressionsanalysen zeigen, dass die positiven Sentimentwerte vor allem von den emotionalen Werten *joy, trust, anticipation* getragen werden, die negativen Sentimentwerte dagegen von den emotionalen Werten *anger, disgust, fear, sadness*. Der emotionale Wert *surprise* (Überraschung) scheint negativ mit dem positiven Sentiment korreliert zu sein. Überraschungen können natürlich nicht nur angenehm (positiv) sein, sondern auch unangenehm (negativ). 

```{r message=FALSE, warning=FALSE}
library(jtools)
mpos <- lm(positive ~ joy + trust + anticipation + surprise, 
           data = nrc_sentiment)
summ(mpos)
mneg <- lm(negative ~ anger + disgust + fear + sadness + surprise, 
           data = nrc_sentiment)
summ(mneg)
```


### Variante 3

Im dritten Versuch soll ein Wortliste mit emotionalen Werten zum Einsatz kommen, und zwar das BAWLR. 

```{r message=FALSE, warning=FALSE}
# This lexicons contains values of Emotional valence and arousal ranging from 1 to 5.
# But this extended version contains also binary Emo_Val values (1, -1).
bawlr <- read.delim2("data/BAWLR_utf8.txt", sep = "\t", dec = ",", fileEncoding = "UTF-8", 
                     header = T, stringsAsFactors = T)
# # bawlr$EmoVal <- as.character(bawlr$EmoVal)
# # str(EmoVal)
# bawlr$EmoVal <- gsub('NEG', '-1', bawlr$EmoVal)
# bawlr$EmoVal <- gsub('POS', '1', bawlr$EmoVal)
# bawlr$EmoVal <- as.numeric(bawlr$EmoVal)
bawlr %>% rmarkdown::paged_table()

```

Machen wir zwei Listen, und zwar eine Liste mit positiv bewerteten Wörtern und eine mit negativ bewerteten Wörtern!

```{r message=FALSE, warning=FALSE}
positive.words = bawlr %>% 
  mutate(WORD_LOWER = as.character(WORD_LOWER)) %>% 
  dplyr::select(EmoVal, WORD_LOWER) %>% 
  filter(EmoVal == "POS") %>% 
  dplyr::select(WORD_LOWER) %>% 
  filter(str_detect(WORD_LOWER, "[a-zA-Z]"))

negative.words = bawlr %>% 
  mutate(WORD_LOWER = as.character(WORD_LOWER)) %>% 
  dplyr::select(EmoVal, WORD_LOWER) %>% 
  filter(EmoVal == "NEG") %>% 
  dplyr::select(WORD_LOWER) %>% 
  filter(str_detect(WORD_LOWER, "[a-zA-Z]"))

```

Daraus erstellen wir ein `quanteda` Lexikon, und zwar mit der Funktion  `dictionary()`:

```{r message=FALSE, warning=FALSE}
bawlr_dict = dictionary(list(positive = list(positive.words), negative = list(negative.words)))

```

Wir verwenden eine in vorherigen Abschnitten gebildete Matrix (dfm) mit Lexikoneinheiten (Lemmas), da das *bawlr_dict*-Wörterbuch nur die Grundform der Lemmata enthält.

```{r message=FALSE, warning=FALSE}
matrika_lemmas = dfm(matrika_lem, tolower = TRUE)

result = matrika_lemmas %>% 
  dfm_lookup(bawlr_dict) %>% 
  convert(to = "data.frame") %>% 
  as_tibble
result

```

Wir können die Gesamtwortlänge hinzufügen, wenn das Ergebnis in Bezug auf die Länge der Texte normalisiert werden soll.

```{r message=FALSE, warning=FALSE}
result = result %>% mutate(length=ntoken(matrika_lemmas))
result

```

Normalerweise wollen wir den *Gesamtstimmungswert* berechnen. Dafür gibt es mehrere Möglichkeiten: z.B.    
- die negativen Werte von den positiven zu subtrahieren und dann die Differenz durch die Summe der beiden Kategorien zu dividieren,    
- die negativen Werte von den positiven Werten zu subtrahieren und dann die Differenz durch die Länge der Texte zu dividieren. 

Wir können auch den *Grad der Subjektivität* berechnen, d.h. wie viele emotionale Werte insgesamt ausgedrückt werden: 

```{r message=FALSE, warning=FALSE}
result = result %>% mutate(sentiment1=(positive - negative) / (positive + negative))
result = result %>% mutate(sentiment2=(positive - negative) / length)
result = result %>% mutate(subjektivnost=(positive + negative) / length)
result %>% rmarkdown::paged_table()

```

Der Subjektivitätsgrad scheint demnach im *Prozess* etwas höher zu sein als im *Tom Sawyer*. 


#### Farbliche Sentimentmarkierung

Das Programm `corpustools` ermöglicht die farbliche Kodierung der Textwörter gemäß den Stimmungswerten, die im Sentimentlexikon verzeichnet sind.

Der erste Schritt besteht darin, einen `tcorpus` anzulegen.

```{r message=FALSE, warning=FALSE}
library(corpustools)
t = create_tcorpus(txt, doc_column="doc_id")

```

Der zweite Schritt ist eine Wortrecherche im `tcorpus`:

```{r message=FALSE, warning=FALSE}
t$code_dictionary(bawlr_dict, column = 'bawlr')
t$set('sentiment', 1, subset = bawlr %in% c('positive','neg_negative'))
t$set('sentiment', -1, subset = bawlr %in% c('negative','neg_positive'))

```

Dies ermöglicht die Anzeige der farbkodierten Texte im "Viewer"-Fenster:

```{r message=FALSE, warning=FALSE}
browse_texts(t, scale='sentiment')

```

Der farbkodierte Text kann in einem Webbrowser angezeigt und als HTML-Datei gespeichert werden: 

```{r message=FALSE, warning=FALSE}
browse_texts(t, scale='sentiment', filename = "sentiment_prozess_tom.html", 
             header = "Sentiment in Kafkas Prozess und Twains Tom Sawyer")

```
