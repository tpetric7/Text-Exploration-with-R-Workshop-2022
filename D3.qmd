# Socialna omrežja in podnapisi {#sec-delavnica3}

```{r}
#| echo: false
#| fig-keep: 'all'
#| out-width: "100%"
#| fig-link: "https://theconversation.com/the-math-behind-trumps-tweets-100314"
im <- magick::image_read("https://images.theconversation.com/files/228680/original/file-20180721-142432-io1eym.png?ixlib=rb-1.1.0&q=45&auto=format&w=754&h=754&fit=crop&dpr=1")
im <- magick::image_background(im, color = "white")
magick::image_write(im, "pictures/twitter_trump_network.png")

knitr::include_graphics("pictures/twitter_trump_network.png", 
                        dpi = 300)
```

Primer preprostega animiranega zemljevida, ki prikazuje časovni potek čivkov o smrti bobnarja rockovske skupine *Rolling Stones*, *Charlieja Wattsa*. Zajeti so bili čivki v devetdnevnem obdobju v angleščini, nemščini in slovenščini. [Petrič 2022](https://tpetric7.github.io/raj2022-book/rolling-stones-auf-twitter.html#zeitlicher-geographischer-verlauf)     

```{r}
knitr::include_graphics("pictures/rolling_stones_tweets_animated.gif")
```


## Nalaganje knjižnic

```{r}
#| warning: false
#| message: false
library(tidyverse)
library(tidytext)
library(readxl)
library(writexl)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(rtweet)
library(Twitmo)
library(ndjson)
library(plotly)
library(lubridate)
library(igraph)
library(textfeatures)
```

## Poizvedbe v omrežju *Twitter*

### Preverjanje pristnosti (avtentikacija)

Za poizvedbe se morate najprej prijaviti na Twitterjevem programskem vmesniku (*API*).

Za poizvedbe je treba ustvariti *Twitter App* <apps.twitter.com>. Opis: gl. <https://mkearney.github.io/nicar_tworkshop/#10>. 

Najpreprostejša prijava z lastno Twitterjevo aplikacijo poteka z naslednjim ukazom: 

```{r}
#| message: false
#| warning: false
rtweet::auth_setup_default()
```

```{r}
#| eval: false
#| include: false

# auth <- rtweet_bot()
bearer_token <- jsonlite::fromJSON("d:/Users/teodo/Documents/R/Twitter/credentials/twitter_bearer_token_tp.json")

bearer_token <- rtweet_app(bearer_token$access_token)
```

`R` pošilja vaš čivk. 

```{r}
#| eval: false
post_tweet(paste0("My first tweet with #rtweet #rstats at ", Sys.time()))
```


Nekaj koristnih naslovov s "kuharskimi" recepti za poizvedovanje v socialnem omrežju *Twitter*:   

<https://cran.r-project.org/web/packages/rtweet/vignettes/rtweet.html>   

<https://mkearney.github.io/nicar_tworkshop/#1>

<https://www.gbailey.uk/twitter_workshop/pt2_collection.html>

<https://rpubs.com/vivvi87/rtweet_notes>

<https://predictivehacks.com/how-to-get-twitter-data-using-r/>


### Sprotne poizvedbe

#### Prijatelji

Pridobi id prijateljev navedene osebe s funkcijo `get_friends()` knjižnice `rtweet`, tj. katerim uporabnikom navedena oseba sledi.   

```{r}
#| eval: false

prijatelji_jj <- get_friends("jjansasds")
prijatelji_al <- get_friends("anzelog")

saveRDS(prijatelji_jj, "data/twitter/politiki/prijatelji_jansa.rds")
saveRDS(prijatelji_al, "data/twitter/politiki/prijatelji_logar.rds")
```

Odpiramo shranjeno poizvedbo. 

```{r}
prijatelji_jj <- read_rds("data/twitter/politiki/prijatelji_jansa.rds")
prijatelji_al <- read_rds("data/twitter/politiki/prijatelji_logar.rds")

dim(prijatelji_jj)
dim(prijatelji_al)
```
Janez Janša sledi 4060 uporabnikom, Anže Logar pa 959 uporabnikom. 

Kateri prijatelji so skupni oz. katerim uporabnikom sledita oba politika? To lahko hitro ugotovimo s funkcijo `intersect()`.

```{r}
prijatelji_skupni <- 
  intersect(prijatelji_jj$to_id, prijatelji_al$to_id) %>% 
  as_tibble_col(column_name = "skupni_prijatelji")
dim(prijatelji_skupni)
```

Mnogim uporabnikom, ki jim sledi *Anže Logar* (tj. 557 od 959), sledi tudi *Janez Janša*.

Kateri so ti skupni prijatelji? To lahko ugotovimo s funkcijo `lookup_users()`.

```{r}
#| eval: false

prijatelji_jj_data <- lookup_users(prijatelji_jj$to_id)
prijatelji_al_data <- lookup_users(prijatelji_al$to_id)

saveRDS(prijatelji_jj_data, "data/twitter/politiki/prijatelji_jansa_data.rds")
saveRDS(prijatelji_al_data, "data/twitter/politiki/prijatelji_logar_data.rds")
```

Odpiramo shranjeno poizvedbo. 

```{r}
prijatelji_jj_data <- read_rds("data/twitter/politiki/prijatelji_jansa_data.rds")
prijatelji_al_data <- read_rds("data/twitter/politiki/prijatelji_logar_data.rds")

dim(prijatelji_jj_data)
dim(prijatelji_al_data)
```

```{r}
prijatelji_skupni_data <- 
  intersect(prijatelji_jj_data$name, prijatelji_al_data$name) %>% 
  as_tibble_col(column_name = "imena_skupnih_prijateljev")
dim(prijatelji_skupni_data)
```

Poizvedba nam je dala 562 skupnih prijateljev. Tu so imena:  

```{r}
prijatelji_skupni_data %>% 
  rmarkdown::paged_table()
```

Skupni prijatelji so lahko npr. povezani z izbiro pogovorne tematike, značilnih izrazov itd. 

#### Sledilci

Ukaz `get_followers()` nam omogoča najti sledilce navedenega uporabnika. 

```{r}
#| eval: false

npmusar_sledilci <- get_followers("nmusar", 
                                  retryonratelimit = TRUE)
npmusar_sledilci_data <- lookup_users(npmusar_sledilci$from_id)

saveRDS(npmusar_sledilci, "data/twitter/politiki/npmusar_sledilci.rds")
saveRDS(npmusar_sledilci_data, "data/twitter/politiki/npmusar_sledilci_data.rds")
```

Odpiramo shranjeno poizvedbo. 

```{r}
npmusar_sledilci_data <- read_rds("data/twitter/politiki/npmusar_sledilci_data.rds")

dim(npmusar_sledilci_data)
```

```{r}
npmusar_sledilci_data %>% rmarkdown::paged_table()
```

```{r}
npmusar_sledilci_data %>% 
  filter(str_detect(name, "Janša")) %>% 
  rmarkdown::paged_table()
```

#### Iskanje čivkov

Funkcija `search_tweets()` nam vrne tabelo čivkov z ozirom na tematsko vprašanje. 

```{r}
#| eval: false

q <- "ukrajina"

ukraijina <- search_tweets(q = q, 
                        n = 1000,
                        # token = rtweet_app(bearer_token$access_token),
                        include_rts = FALSE,
                        `-filter` = "replies",
                        lang = "sl"
                        )

saveRDS(ukraijina,
        "data/twitter/politiki/ukraijina_tweets_sl.rds")
```

Odpiramo shranjeno poizvedbo. 

```{r}
ukraijina <- read_rds(
  "data/twitter/politiki/ukraijina_tweets_sl.rds")

dim(ukraijina)
```

#### Časovnica

Privzeto 100 čivkov, zgornja meja n = 3200. Še več, če pridobil bearer_token. 

```{r}
#| eval: false

fajon_timeline <- get_timeline("tfajon", n = 3200)

saveRDS(fajon_timeline, "data/twitter/politiki/fajon_timeline.rds")
```

Odpiramo shranjeno poizvedbo. 

```{r}
fajon_timeline <-
  read_rds("data/twitter/politiki/fajon_timeline.rds")
```


```{r}
tline_plot_fajon <- fajon_timeline |> 
  filter(created_at > "2022-01-01") |> 
  ts_plot(by = "weeks", trim = 1L, color = "magenta") +
  geom_point() +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of Twitter statuses by Tanja Fajon",
    subtitle = "Twitter status counts aggregated by weeks from 2022",
    caption = "Source: Data collected from Twitter's REST API via rtweet"
  )

library(plotly)
ggplotly(tline_plot_fajon)
```

#### Priljubljeni čivki

Pridobiti želimo 10 zadnjih priljubljenih čikov Tanje Fajon in Mateja Tonina. 

```{r}
#| eval: false

favorit_fajon <- get_favorites("tfajon", n = 10)
favorit_tonin <- get_favorites("MatejTonin", n = 10)

saveRDS(favorit_fajon, "data/twitter/politiki/fajon_favorit.rds")
saveRDS(favorit_tonin, "data/twitter/politiki/tonin_favorit.rds")
```

Odpiramo shranjeno poizvedbo.  

```{r}
favorit_fajon <-
  read_rds("data/twitter/politiki/fajon_favorit.rds")
favorit_tonin <-
  read_rds("data/twitter/politiki/tonin_favorit.rds")
```


```{r}
favorit_fajon %>% 
  select("text", "created_at", "id_str") %>%
  rmarkdown::paged_table()

favorit_tonin %>% 
  select("text", "created_at", "id_str") %>%
  rmarkdown::paged_table()
```

#### Poglej izbrane čivke

Kontroverzni čivki Donalda Trumpa. 

```{r}
#| eval: false


## status IDs
status_ids <- c("947235015343202304", "947592785519173637", 
  "948359545767841792", "832945737625387008")
## lookup tweets
twt <- lookup_tweets(status_ids)
```


Kontroverzni čivki Janeza Janša. 

Jansa's tweet, Carpenter's reaction and Fazlic commenting on Carpenter's reaction

```{r}
# status_id = "1319567043218296833" (Jansa's suport from Trump)
status = "1320064582170398722" # Comment on Carpenter's tweet by Damir Fazlić
tw1 <- lookup_statuses(status)
tw1

```

Janša's provocative tweet

```{r}
# status_id = "1319567043218296833" (Jansa's suport for Trump)
status = "1319567043218296833"
tw <- lookup_statuses(status)
tw

```

```{r}
# status_id = "1324075189802512384" (Jansa: lefties can't stand Trump is winner)
# status_id = "1323913419200864256" (Jansa: Trump is winner)
status2 = "1323913419200864256"
tw2 <- lookup_statuses(status2)
tw2

```

Poizvedba s 5. novembra 2020.

```{r}
#| eval: false

# status_id = "1323913419200864256" (Jansa's claim that Trump is winner of elections)
tweets_replies_jj_support2 <- search_tweets(q = "1323913419200864256", 
                        n = 18000,
                        include_rts = FALSE,
                        `-filter` = "replies",
                        lang = "en")
```

Shranjena poizvedba s 5. novembra 2020.  

```{r}
tweets_replies_jj_support2 <- read_rds("data/twitter/politiki/replies_to_jj_support_for_trump.rds")

tweets_replies_jj_support2 %>% 
  select(text, created_at, name, screen_name) %>% 
  rmarkdown::paged_table()
```

#### Streaming -- Tok v živo

```{r}
#| eval: false

library(Twitmo)
library(ndjson)

## random sample
nakljucen_tok <- get_tweets(method = 'stream', 
           location = "SLV", 
           timeout = 30, 
           file_name = "data/slv_tweet_stream.json")
```

```{r}
nakljucen_tok <- ndjson::stream_in("data/slv_tweet_stream.json")

nakljucen_tok[1:10] %>% 
  select(text)
```


```{r}
#| eval: false

st <- stream_tweets(q = "Soccer World Cup", timeout = 30,
                    file_name = "data/stream_tweets32907e436248.json")
```
```{r}
st <- ndjson::stream_in("data/stream_tweets32907e436248.json")

st %>% 
  slice_sample(n = 10) %>% 
  select(text)
```

```{r}
#| eval: false

iran <- stream_tweets(q = "Iran", timeout = 30, 
                    file_name = "data/iran.json")
```

```{r}
iran <- ndjson::stream_in("data/iran.json")

iran %>% 
  slice_sample(n = 10) %>% 
  select(text)
```

Druge rtweet funkcije:   


    search_users()
    lookup_users()
    get_trends()
    stream_tweets()
    lists_members()
    lists_statuses()

    lookup_coords()
    tweet_shot()
    post_*()
    ts_data()
    lat_lng()
    emojis
    stopwordslangs

#### Analiza omrežja

Analiza in upodobitev omrežij s knjižnico `igraph`:   
https://kateto.net/netscix2016.html 

```{r}
#| eval: false

## get friends of multiple accounts
fds <- get_friends(c("nmusar", "anzelog", "tfajon"), 
                   retryonratelimit = TRUE)

## frequency count of accounts followed by the users queried above
tbl <- table(fds$from_id)

## subset fds data to only those followed by 5 or more
fds3 <- subset(fds, from_id %in% names(tbl[tbl > 15L]))

fds3_names <- lookup_users(fds3$to_id)
fds3_names <- fds3_names %>% 
  rename(to_id = id_str)
fds3_names <- fds3_names %>% 
  select(to_id, name)

fds3 <- fds3 %>% left_join(fds3_names, by = "to_id")
fds3 <- fds3 %>% select(1,3,2)

## convert fds3 to matrix
mat <- as.matrix(fds3)
saveRDS(mat, "data/mat.rds")

fds3 <- fds3 %>% select(1,2)
## convert fds3 to matrix
mat <- as.matrix(fds3)
saveRDS(mat, "data/mat.rds")

## convert to graph object
mat <- igraph::graph_from_edgelist(mat, directed = TRUE)
saveRDS(mat, "data/mat_igraph.rds")
```

```{r}
mat <- read_rds("data/mat_igraph.rds")
```

```{r}
head(mat)
```

```{r}
mat

# mat[]
```

```{r}
V(mat)$name[1:5]

```

```{r}
## plot network
# plot(mat)

plot(mat, layout = layout_nicely(mat), 
     edge.arrow.size=.25, vertex.color="gold", vertex.size=5, 
     vertex.frame.color="gray", # vertex.shape = "none", 
     vertex.label.color=c("pink","skyblue"), edge.curved=0.2, 
     vertex.label.cex=0.3, vertex.label.dist=5)
```

```{r}
mats <- simplify_and_colorize(mat)
mats <- delete.edges(mats, which(E(mats)$weight <= 0.1)) # condition
mats <- delete.vertices(mats, which(degree(mats) <= 1)) # condition
plot(mats, layout = layout_nicely(mats), vertex.size=5)

mats <- simplify(mat, remove.multiple = T, 
                              remove.loops = F, 
                              edge.attr.comb=c(weight="sum", 
                                               type="ignore"))
mats <- delete.edges(mats, which(E(mats)$weight <= 0.1)) # condition
mats <- delete.vertices(mats, which(degree(mats) <= 1)) # condition
plot(mats, vertex.label.dist=1.5, vertex.label = NA)
```

Pretvori povezave (edge) in vozlišča (vertex) v tabele (dataframe). 

```{r}
as_data_frame(mat, what="edges") %>% rmarkdown::paged_table()

as_data_frame(mat, what="vertices") %>% rmarkdown::paged_table()
```


#### Sentimentna analiza

```{r}
#| eval: false

## function to round time (created_at)
round_time <- function(x, secs) as.POSIXct(hms::round_hms(x, secs))

## function to calculate sentiment scores
sent_scores <- function(x) syuzhet::get_sentiment(plain_tweets(x)) - .5

## calc data set with sentiment variable
tt_sent <- tt %>%
  mutate(days = round_time(created_at, 60 * 60 * 24),
    sentiment = sent_scores(text))

## aggregate by rounded time interval
tt_sent  %>% 
  group_by(days) %>%
  summarise(sentiment = sum(sentiment, na.rm = TRUE)) %>%
  ggplot(aes(x = weeks, y = sentiment)) +
  geom_point(aes(colour = sentiment > 0)) + 
  geom_smooth(method = "loess", span = .2) + 
  scale_color_manual(values = c("#dd3333", "#22aa33")) + 
  geom_hline(yintercept = 0, linetype = 2, colour = "#000000cc") + 
  theme_minimal(base_family = "Helvetica Neue")
```

```{r}
#| eval: false

## aggregate text features by the hour
tt_tft <- tt %>% 
  filter(created_at > "2010-12-31") %>%
  mutate(hours = round_time(created_at, 60 * 60)) %>%
  group_by(hours) %>%
  textfeatures::textfeatures() %>%
  print()
```

```{r}
#| eval: false

## plot [smoothing] text features over time
tt_tft %>%
  filter(hours > "2014-05-31") %>%
  mutate_if(is.numeric, function(x) scale(x)[, 1]) %>%
  gather(feature, n, -hours) %>%
  ggplot(aes(x = hours, y = n, colour = feature)) + 
  geom_vline(xintercept = as.POSIXct("2016-11-08"),
    colour = "#aa000077", size = .9, linetype = 2) +
  geom_smooth(method = "loess", span = .1) + 
  facet_wrap(~ feature, ncol = 6)
```


------

### Že opravljene poizvedbe

#### Časovnice

Časovnice slovenskih politikov na Twitterju.

```{r}
seznam_polit <- list.files(path = "data/twitter/politiki/",
                           pattern = "_2022-12-16.rds",
                           full.names = TRUE)

seznam_imen <- str_replace(
  seznam_polit, 
  pattern = "(^.*politiki/)(.*)(_time.*.rds$)",
  replacement = "\\2")

politiki_df <- NULL

for(i in 1:length(seznam_polit)){
  x <- read_rds(seznam_polit[i]) %>% 
    mutate(author = seznam_imen[i])
  politiki_df <- bind_rows(politiki_df, x)
}

write_xlsx(politiki_df, 
           "data/twitter/politiki/twitter_politiki_df.xlsx")

dim(politiki_df)
```

```{r}
politiki_df %>%
  group_by(created_at, author) %>% 
  count(author, sort = TRUE) %>% 
  ungroup() %>% 
  group_by(author) %>% 
  filter(created_at > "2018-01-01") %>%
  ts_plot("weeks", color = "blue") +
  labs(x = "", y = "") +
  facet_wrap(~ author, scales = "free_y")
```

```{r}
p1 <- politiki_df %>% 
  filter(created_at > "2018-01-01") %>%
  group_by(author) %>% 
  ts_plot("days", color = "red") +
  theme_minimal() +
  theme(plot.title = ggplot2::element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of Twitter statuses",
    subtitle = "Twitter status (tweet) counts aggregated using week intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  ) +
  facet_wrap(~ author, scales = "free_y")

library(plotly)
ggplotly(p1)
```

#### Jeziki

Preštej jezike:

```{r}
politiki_df %>% 
  count(lang, sort = TRUE)
```

Izberi samo čivke v slovenščini:

```{r}
politiki_df <- politiki_df %>% 
  filter(lang == "sl")
dim(politiki_df)
```

#### Vsota besed (tokens)

Vsota besed (tokens), najdaljši in najkrajši čivki.

```{r}
politiki_tokens_sum <- politiki_df %>% 
  group_by(author) %>% 
  summarise(tokens_sum = sum(ntoken(text)), 
            tokens_min = min(ntoken(text)),
            tokens_max = max(ntoken(text)),
            char_max = max(nchar(text))) %>% 
  arrange(-tokens_sum)

politiki_tokens_sum %>% rmarkdown::paged_table()
```

#### Povprečna dolžina čivka

Koliko besed (z ločili in simboli vred) čivkajo politiki v povprečju?

```{r}
politiki_token_mean <- politiki_df %>% 
  group_by(author) %>% 
  summarise(median_token = median(ntoken(text)), 
            mean_token = mean(ntoken(text)),
            sd_token = sd(ntoken(text))) %>% 
  arrange(-mean_token)
politiki_token_mean
```

#### Povedi

```{r}
politiki_povedi <- politiki_df %>% 
  unnest_tokens(poved, text, "sentences")
```

```{r}
politiki_povedi %>% 
  group_by(author) %>% 
  summarise(median_sent = median(ntoken(poved)), 
            mean_sent = mean(ntoken(poved)),
            sd_sent = sd(ntoken(poved)),
            char_sent = mean(nchar(poved))) %>% 
  arrange(-mean_sent)
```

#### Ngrami

```{r}
politiki_ngrams <- politiki_df %>% 
  unnest_tokens(ngram, text, "ngrams", n = 2)
```

```{r}
politiki_ngrams %>% 
  group_by(author) %>% 
  count(ngram, sort = TRUE)

```

#### Besede

```{r}
politiki_besede <- politiki_df %>% 
  unnest_tokens(word, text, "words")
```

```{r}
politiki_besede %>% 
  group_by(author) %>% 
  count(word, sort = TRUE)

```

-------

    Pogovori o ukrepih proti covidu

```{r}
#| eval: false
#| include: false

# naredi seznam datotek v mapi
seznam_corona <- list.files(path = "data/twitter/corona/", 
                              pattern = ".xlsx", 
                              full.names = TRUE)
# odpri in vtakni vse relevantne xlsx datoteke v podatkovni niz
corona_df <- map_dfr(seznam_corona, read_xlsx)
dim(corona_df)
```

    Pogovori o vojni v Ukrajini

```{r}
#| eval: false
#| include: false

# naredi seznam datotek v mapi
seznam_ukrajina <- list.files(path = "data/twitter/ukrajina/", 
                              pattern = ".xlsx", 
                              full.names = TRUE)
# odpri in vtakni vse relevantne xlsx datoteke v podatkovni niz
ukrajina_df <- map_dfr(seznam_ukrajina, read_xlsx)

dim(ukrajina_df)
```

```{r}
#| eval: false
#| include: false

ukrajina_df %>% 
  count(lang)
```


## Označevanje in lematizacija besed v podnapisih

### Prilaganje podnapisov

Za jezikovno gradivo izberemo podnapise filma *Avatar*, ki so bili prevedeni v slovenščino. Primerjave podnapisov v več jezikih najdete v spletni knjigi [Petrič 2022](https://tpetric7.github.io/raj2022-book/untertitel.html)  

```{r}
avatar_slv <- read_lines("data/sub/Avatar_slv.txt")
head(avatar_slv, 10)
```

Podnapisi nimajo primerne oblike za jezikovno analizo. Potrebujemo le vrstice z dialogom. Le-te želimo pretvoriti v obliko tabele, kar nam omogoča oblikovanje in analizo besedila s programskimi funkcijami `tidyverse` ali `quanteda`. 

```{r}
c1 = avatar_slv %>% 
  as_tibble() %>% 
  mutate(row_tc = row_number()) %>% 
  filter(str_detect(value, "-->")) %>% 
  rename(timecode = value)

c2 = avatar_slv %>% 
  as_tibble() %>% 
  mutate(row_id = row_number()) %>% 
  filter(str_detect(value, "[a-zA-Z]")) %>% 
  rename(text = value) %>% 
  mutate(text = str_replace(text, "\\<i\\>", "")) %>% 
  mutate(text = str_replace(text, "\\</i\\>", "")) %>% 
  mutate(language = "slv")

# avatar_slv = bind_cols(a1,a2)
#   select(timecode, text) %>% 
#   separate(timecode, into = c("start", "end"), sep = "\\-\\-\\>")
# tail(avatar_slv)

avatxt = c2 %>% 
  # opcija: dodaj stolpec s prvotnimi številkami vrstic v podnapisih
  mutate(sentence_id = row_number()) %>% 
  # regex: odstrani pomišljaj pred besedo
  mutate(text = str_replace(text, "(–)([a-zA-ZčšžČŠŽ]+)", "\\2"))

head(avatxt)
```

### Uporaba udpipe

Za označevanje (POS Tagging) in lematizacijo besedilnih enot nam je več knjižnic na razpolago, najpogosteje uporabljeni pa sta `spacyr` in `udpipe`. Knjižnica `spacyr` zahteva namestitev programskih knjižnic v jeziku `Python` in za zdaj ne omogoča označevanja slovenskih besednih oblik. To sta dva tehtna razloga za uporabo knjižnice `udpipe`. 

Najprej si moramo priskrbeti jezikovni model za slovenščino z interneta. V sledečem programskem odstavku `R` preverja, ali je navedena datoteka (*destfile* = ...) že v delovni mapi. Če je še ni, potem jo potegne s privzetega strežnika na računalnik. Če je jezikovni model že v delovni mapi, preskoči nalaganje z interneta in naloži model z diska v delovni pomnilnik računalnika. 

```{r message=FALSE, warning=FALSE}
library(udpipe)
destfile = "slovenian-ssj-ud-2.5-191206.udpipe"

if(!file.exists(destfile)){
   jezikovni_model <- udpipe_download_model(language = "slovenian")
   udmodel_sl <- udpipe_load_model(jezikovni_model$file_model)
   } else {
  file_model = destfile
  udmodel_sl <- udpipe_load_model(file_model)
}

```

Naslednji programski odstavek vsebuje funkcijo tipa *"Sam svoj mojster"* `r emo::ji("thinking")`. Lastnoročna izdelana funkcija poskrbi za to, da tabeli dodamo več stolpcev, ki vsebujejo označbe besednih oblik in količine za merjenje dolžine besed ali povedi. 

```{r message=FALSE, warning=FALSE}
tokenize_annotate = function(tbl){
  tbl %>% 
  unnest_tokens(word, token, drop = F) %>% 
  cbind_morphological(term = "feats",  
                      which = c("PronType","NumType","Poss","Reflex",
                                "Foreign","Abbr","Typo",
                                "Gender","Animacy","NounClass",
                                "Case","Number","Definite","Degree",
                                "VerbForm","Person","Tense","Mood",
                                "Aspect","Voice","Evident",
                                "Polarity","Polite","Clusivity")) %>% 
  mutate(txt = str_replace_all(sentence, "[:punct:]", "")) %>% 
  mutate(sentlen = quanteda::ntoken(txt)) %>% 
  mutate(syllables = nsyllable::nsyllable(txt)) %>% 
  mutate(types = quanteda::ntype(txt)) %>% 
  mutate(wordlen = syllables/sentlen) %>% 
  mutate(ttr = types/sentlen) %>% 
  select(-txt, -feats)
}

```

Po potrebi lahko poskrbimo tudi za ohranitev stolpcev iz prvotne tabele, ki bi se sicer v procesu označevanja besedilnih enot izgubile.

```{r}
#| eval: false

# keep columns of interest
# these columns will be fed into doc_id of udpipe
# afterwards the doc_id column will be separated
keepcol <- paste(avatxt$row_id, avatxt$language, 
                 sep = "_")
```

Programski odstavek, ki poskrbi za označevanje besednih oblik. 

```{r message=FALSE, warning=FALSE}
#| eval: false

udp <- udpipe_annotate(
  # jezikovni model za slovenščino
  object = udmodel_sl, 
  # izbrani podnapisi
  x = as.character(avatxt$text),
  # opcija: prilagodimo doc_id
  doc_id = as.character(keepcol),
  # pokaže potek označevanja, če TRUE
  trace = TRUE)

udp <- as.data.frame(udp) 
  
udpiped <- udp %>% 
  # gornja lastnoročna izdelana funkcija
  tokenize_annotate() %>% 
  # oblikovanje vsebine stolpcev
  mutate(token_id = as.numeric(token_id),
         head_token_id = as.numeric(head_token_id)) %>% 
  # razdeli stolpec doc_id na več novih
  separate(doc_id, 
           into = c("row_id", "language"), 
           sep = "_", extra = "merge")

# shrani tabelo v varčni obliki (le R zna brati take datoteke)
saveRDS(udpiped, "data/avatar_slv_udpiped.rds")
```

Da malo pospešimo delo, bomo kar naložili že pripravljeno datoteko z diska. 

```{r}
udpiped <- read_rds("data/avatar_slv_udpiped.rds")
head(udpiped, 3) %>% rmarkdown::paged_table()
```

### Besedne vrste

Katere besedne vrste in koliko pojavnic je program prepoznal?

```{r}
bes_vrste <- udpiped %>% 
  count(upos, sort = T) %>% 
  mutate(pct = round(100*n/sum(n), 2))
bes_vrste %>% 
  rmarkdown::paged_table()
```


```{r}
bes_vrste %>% 
  # izloči ločila in neznano besedno vrsto
  filter(!upos %in% c("X", "PUNCT")) %>% 
  # razvrščaj besedne vrste po odstotkih
  mutate(upos = fct_reorder(upos, pct)) %>% 
  # pct delimo s 100, ker bomo spodaj uveljavili "percent_format"
  ggplot(aes(pct/100, upos, color = upos)) +
  # nariši črte
  geom_segment(aes(x = 0, xend = pct/100, y = upos, yend = upos)) +
  # dodaj pike oz. bucke
  geom_point(size = 3) +
  # uporabi oblikovalno predlogo
  theme_light() +
  # uveljavi oblikovanje x osi (enota je odstotek)
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  # imena osi niso potrebna
  labs(x = "", y = "") +
  # odstrani legendo
  theme(legend.position = "none")
```

### Samostalniki

Kateri samostalniki so bili najpogosteje uporabljeni v podnapisih?

```{r}
samostalniki <- udpiped %>% 
  # izberi le samostalnike
  filter(upos == "NOUN") %>% 
  # opcija: samo male črke
  # mutate(lemma = tolower(lemma)) %>% 
  # preštej in razvrščaj
  count(lemma, sort = T) %>% 
  # dodaj stolpec z odstotki
  mutate(pct = round(100*n/sum(n), 2))

samostalniki %>% 
  rmarkdown::paged_table()
```

```{r}
samostalniki %>% 
  # razvrščaj slovarske enote glede na odstotkovno pogostnost
  mutate(lemma = fct_reorder(lemma, pct)) %>% 
  # pokaži samo prvih 20
  head(20) %>% 
  ggplot(aes(pct/100, lemma, color = lemma)) +
  # nariši črto
  geom_segment(aes(x = 0, xend = pct/100, y = lemma, yend = lemma)) +
  # nariši točko oz. bucko
  geom_point(size = 3) +
  # izberi oblikovalno predlogo
  theme_light() +
  # x os naj ima odstotke kot enoto
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  # brez imen osi
  labs(x = "", y = "") +
  # brez legende
  theme(legend.position = "none")
```

S funkcijo `xray()` knjižnice `quanteda.textplots` lahko pokažemo, v katerih vrsticah podnapisov (ki ima 1606 vrstic) se pojavljajo izbrane besede, npr. imena glavnih likov filma. 

```{r}
textplot_xray(
  kwic(avatxt %>% pull(text), 
                 pattern = c("Grace","Tsu'tey")), 
  scale = "relative")

```

V katerih oblikah in v katerih vrsticah podnapisov nastopa slovarska enota (lemma) *brat*?

```{r}
udpiped %>% 
  # izberi vrstice s slovarsko enoto "brat"
  filter(lemma == "brat") %>% 
  # izberi stolpce za prikaz
  select(sentence, token, lemma, upos, xpos, dep_rel) %>% 
  rmarkdown::paged_table()
```

Z naslednjo poizvedbo želimo najti samostalnike v množinski obliki. 

```{r}
#Find all plural nouns (tokens)
udpiped %>% 
  # nastavimo ustrezne filtre za izbor vrstic tabele
  filter(language == "slv" & # tu nepotrebno, ker imamo samo slv
           upos == "NOUN" & 
           morph_number == "Plur") %>% 
  select(sentence, token, lemma, upos, xpos, morph_number) %>% 
  rmarkdown::paged_table()
```

V sledečem programskem odstavku sestavljamo tabelo o pogostnosti edninskih množinskih in dvojinskih oblik samostalnikov. 

```{r}
udpiped %>% 
  # izberemo relevantne stolpce
  dplyr::select(language, token, lemma, upos, morph_number) %>% 
  # tu ni potrebno, ker imamo samo slv podnapise
  group_by(language) %>% 
  # izbere le vrstice s samostalniki
  filter(upos == "NOUN") %>% 
  # preštej vrstice v stolpcu o številu
  count(morph_number) %>% 
  # preoblikuj tabelo iz dolge v široko obliko (lažje beremo)
  pivot_wider(names_from = language, values_from = n) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  # v vseh stolpcih zamenjaj NA z ničlo
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  # v stolpcu zamenjaj črkovni niz "0" z opisno vrednostjo "Unknown"
  mutate(morph_number = 
           str_replace(morph_number, "0", "Unknown")) %>% 
  # uredi stopnje v kategoriji število po zaznamovanosti
  mutate(morph_number = 
           fct_relevel(
             morph_number, levels =
                         c("Sing","Plur","Dual","Unknown"))) %>% 
  # dodal naknadno: odstotke
  mutate(pct = round(100*slv/sum(slv), 2)) %>% 
  # razvrščaj po stopnjah kategorije število
  arrange(morph_number) %>% 
  rmarkdown::paged_table()
```

### Pridevniške oblike 

```{r}
udpiped %>% 
  # grupiranje tu ni potrebno, ker imamo samo slv podnapise
  group_by(language) %>% 
  # izbor jezikov tu ni potreben
  filter(language == "eng" | 
           language == "deu" | language == "slv") %>% 
  # izberi le vrstice s pridevniki
  filter(upos == "ADJ") %>% 
  # preštej stopnjevalne oblike in razvrščaj
  count(morph_degree, sort = TRUE) %>% 
  # dodaj stolpec z odstotki
  mutate(pct = round(100*n/sum(n),2)) %>% 
  # preoblikuj dolgo tabelo v široko obliko
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  # v vseh stolpcih zamenjaj NA z ničlo
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  # v stolpcu zamenjaj niz "0" z opisom "Unknown"
  mutate(morph_degree = 
           str_replace(morph_degree, "0", "Unknown")) %>% 
  # uredi vrstni red stopnjevalnih stopenj pridevnikov
  mutate(morph_degree = 
           fct_relevel(
             morph_degree, levels =
                         c("Pos","Cmp","Sup","Abs","Unkown"))) %>% 
  # razvrščaj
  arrange(morph_degree) %>% 
  rmarkdown::paged_table()
```


### Skladenjske analize

#### UD shema

Tipološke za označevanje besedilnih enot: *Universal Stanford Dependencies: A cross-linguistic typology* (de Marneffe et al. 2014).

```{r}
knitr::include_graphics("pictures/Screenshot 2021-08-27 at 12-14-22 Universal Dependency Relations.png")
```

#### Dependenčni odnosi

V naslednjem programskem odstavku sestavljamo tabelo za prikaz skladenjskih prvin po kategorijah UD (gl. zgoraj), in sicer na enak način kot v prejšnjih odstavkih (gl. pojasnila tam). 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(language == "eng" | 
           language == "deu" | language == "slv") %>% 
  count(dep_rel, sort = TRUE) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  mutate(dep_rel = 
           str_replace(dep_rel, "0", "Unknown")) %>% 
  rmarkdown::paged_table()
```

*Koren* odvisnostnega odnosa (root of dependency relation) nam pove, ali je bilo zaporedje besed identificirano kot stavek. Običajno se določi s pomočjo (osebne) glagolske oblike v stavku. V eliptičnih stavkih je ena od besednih oblik, ki se pojavi, povezana s korenom.

Iz tabele (gl. *root*, koren) je razvidno, da je v slovenskih podnapisih prepoznanih 1807 stavčnih enot.

Naslednji programski odstavek vsebuje lastnoročno izdelano *funkcijo za risanje dependenčnih odnosov v stavkih*. Prevzeli smo jo od [poišči](poglej), potem pa prilagodili našim potrebam. Spodaj jo bomo videli v akciji. 

```{r}
library(igraph)
library(ggraph)
library(ggplot2)

plot_annotation <- function(x, size = 3){
  stopifnot(is.data.frame(x) & all(c("sentence_id", "token_id", "head_token_id", "dep_rel", "token_id", "token", "lemma", "upos", "xpos", "feats") %in% colnames(x)))
  x <- x[!is.na(x$head_token_id), ]
  x <- x[x$sentence_id %in% min(x$sentence_id), ]
  edges <- x[x$head_token_id != 0, c("token_id", "head_token_id", "dep_rel")]
  edges$label <- edges$dep_rel
  g <- graph_from_data_frame(edges,
                             vertices = x[, c("token_id", "token", "lemma", "upos", "xpos", "feats")],
                             directed = TRUE)
  windowsFonts("Arial Narrow" = windowsFont("Arial"))
  ggraph(g, layout = "linear") +
    geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20),
                  arrow = grid::arrow(length = unit(4, 'mm'), ends = "last", type = "closed"),
                  end_cap = ggraph::label_rect("wordswordswords"),
                  label_colour = "red", check_overlap = TRUE, label_size = size) +
    geom_node_label(ggplot2::aes(label = token), col = "darkgreen", size = size, fontface = "bold") +
    geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size) +
    theme_graph(base_family = "Arial Narrow") +
    labs(title = "udpipe output", subtitle = "tokenisation, parts of speech tagging & dependency relations")
}
```

Cilj naslednjega programskega odstavka je narisati diagram, ki prikazuje dependenčne odnose med stavčnimi prvinami. Stavek je iz slovenskih podnapisov za film *Avatar*. 

```{r}
#| warning: false
#| message: false

# Slovenian: v navednicah izbrani stavek
mytext = "Začel sem sanjati o letenju" %>% 
  # pretvori gornji niz po kodirni shemi utf8
  enc2utf8()
# izberi stavek in jezikovni model in označuj oblike
x = udpipe(mytext, "slovenian")
# nariši oblikoskladenjsko označen stavek (gl. funkcijo zgoraj)
x3 = plot_annotation(x, size = 3)
x3
```

#### Osebki

Koliko osebkov najdemo s programom? Prilagodimo zgoraj že večkrat sestavljeno tabelo novi poizvedbi. 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(language == "eng" | 
           language == "deu" | language == "slv") %>% 
  filter(str_detect(dep_rel, "nsubj")) %>% 
  count(dep_rel, sort = TRUE) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  mutate(dep_rel = 
           str_replace(dep_rel, "0", "Unknown")) %>% 
  rmarkdown::paged_table()
```

Oglejmo si nekaj vrstic, ki vsebujejo osebek. Prekopiramo še eno tabelo, jo nekoliko prilagodimo novi poizvedbi. 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(language == "slv") %>% 
  filter(str_detect(dep_rel, "nsubj")) %>% 
  ungroup() %>% 
  select(sentence, sentence_id) %>% 
  distinct() %>% 
  head(5) %>% rmarkdown::paged_table()
```

Izberimo vrstico neposredno iz tabele podnapisov in narišimo dependenčne odnose v izbranem stavku. Kako je osebek upodobljen v odvisnostnem diagramu?

```{r}
avatxt %>% 
  filter(language == "slv") %>% 
  filter(str_detect(text, "dobi svojega avatarja")) %>% 
  select(text) %>% rmarkdown::paged_table()

# Slovenian
mytext = "Vsak upravljavec dobi svojega avatarja" %>% enc2utf8()
x = udpipe(mytext, "slovenian")
x3 = plot_annotation(x, size = 3)
x3
```

#### Zaimki in samostalniki

Cilj naslednjega programskega odstavka je ugotoviti, v katerih stavčnih vlogah (osebek, tožilniški (strukturni) predmet, dajalniški predmet ali slovarsko določeni predmet) nastopajo zaimki in samostalniki. Prekopiramo zgoraj že večkrat uporabljeno tabelo in jo prilagodimo novim zahtevam. 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(upos == "NOUN" | upos == "PRON") %>% 
  filter(str_detect(dep_rel, "nsubj|obj|obl")) %>% 
  count(upos, dep_rel) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  arrange(upos) %>% 
  rmarkdown::paged_table()
```

Recikliramo tabelo, tokrat za prikaz števila in deležev osebkov z ozirom na zaimensko ali samostalniško obliko. 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(language == "eng" | 
           language == "deu" | language == "slv") %>% 
  filter(upos == "NOUN" | upos == "PRON") %>% 
  filter(str_detect(dep_rel, "nsubj")) %>% 
  count(upos, dep_rel) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  arrange(upos) %>% 
  rmarkdown::paged_table()
```

Izdelamo lastno (večkrat uporabno) funkcijo za pretvorbo dolgih tabel v široke tabele (ki jih ljudje lažje beremo). 

```{r}
pivot_by_nsubj <- function(tbl) {
  tbl %>% 
    filter(upos == "NOUN" | upos == "PRON") %>% 
    filter(str_detect(dep_rel, "nsubj")) %>% 
    count(upos, dep_rel) %>% 
    group_by(upos) %>% 
    mutate(pct = round(100*n/sum(n),2)) %>% 
    pivot_wider(names_from = upos, values_from = c(n, pct)) %>% 
    mutate_if(is.numeric, ~ replace_na(.x, 0))
    # mutate(across(everything(), ~ replace_na(.x, 0))) 
}

# uporabi gornjo funkcijo
x = udpiped %>% 
  filter(language == "slv") %>% 
  pivot_by_nsubj()

x %>% rmarkdown::paged_table()
```

Oglejmo si nekaj primerov iz podnapisov filma *Avatar*, ki vsebujejo samostalnik ali zaimek v vlogi osebka!

```{r}
udpiped %>% 
  select(sentence, upos, dep_rel, language) %>% 
  filter(language == "slv") %>% 
  filter(upos == "NOUN" | upos == "PRON") %>% 
  filter(str_detect(dep_rel, "nsubj")) %>% 
  rmarkdown::paged_table()
```

Prekopiramo tabelo in jo prilagodimo za prikaz pogostnosti tožilniškega predmeta v samostalniški ali zaimenski obliki. 

```{r}
x = udpiped %>% 
  group_by(language) %>% 
  filter(upos == "NOUN" | upos == "PRON") %>% 
  filter(dep_rel == "obj") %>% 
  count(upos, dep_rel) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  arrange(upos)

x %>% rmarkdown::paged_table()

```

V naslednjem programskem odstavku sestavljamo preglednico za prikaz samostalnikov in zaimkov v vlogi osebka ali tožilniškega predmeta. Tabelo smo spet prekopirali in nekoliko prilagodili novim zahtevam. 

```{r}
udpiped %>% 
  group_by(language) %>% 
  filter(upos == "NOUN" | upos == "PRON") %>% 
  filter(dep_rel == "nsubj" | dep_rel == "obj") %>% 
  count(upos, dep_rel) %>% 
  # group_by(upos) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = language, values_from = c(n, pct)) %>% 
  # mutate(across(everything(), ~ replace_na(.x, 0))) %>% 
  mutate_if(is.numeric, ~ replace_na(.x, 0)) %>% 
  arrange(upos) %>% 
  rmarkdown::paged_table()
```

Lastnoročno izdelana funkcija v naslednjem programskem odstavku za pretvorbo dolge tabele v široko obliko je skoraj enaka kot malo prej sestavljena, razlika je le, da tokrat vključujemo osebke in preme (tožilniške) predmete. 

```{r}
pivot_by_obj <- function(tbl) {
  tbl %>% 
    filter(upos == "NOUN" | upos == "PRON") %>% 
    filter(dep_rel == "nsubj" | dep_rel == "obj") %>% 
    count(upos, dep_rel) %>% 
    group_by(upos) %>%
    mutate(pct = round(100*n/sum(n),2)) %>% 
    pivot_wider(names_from = upos, values_from = c(n, pct)) %>% 
    mutate_if(is.numeric, ~ replace_na(.x, 0))
    # mutate(across(everything(), ~ replace_na(.x, 0)))
}

# uporabi gornjo funkcijo
x = udpiped %>% 
  filter(language == "slv") %>% 
  pivot_by_obj()
  
x %>% rmarkdown::paged_table()
```

S hi kvadrat preizkusom lahko ugotovimo neodvisnost (odvisnost) obeh vzorcev (samostalniškega in zaimenskega). V tem primeru je p vrednost manjša kot mejna vrednost (p < 0,05), tj. statistična značilna. Vzorca nista neodvisna: skladenjska vloga (osebek ali predmet) je povezana s besedno vrsto (zaimek ali samostalnik)

```{r}
# Base-R
hi <- chisq.test(x[,c(2:3)])
hi

# opazovane pogostnosti
hi$observed
# pričakovane pogostnosti po ničelni domnevi (tj. da ni razlike med vzorcema oz. da sta vzorca neodvisna)
hi$expected
```

V spletni knjigi [Petrič 2022](https://tpetric7.github.io/raj2022-book/untertitel.html#syntax-dependenz) je še več skladenjskih analiz s programom `udpipe`, ki si jih lahko ogledate (npr. besedni vrstni red v samostalniških besednih zvezah ali stavkih). 


## Označevanje: YouTube

### Preberi in združi

Sledeče podatkovne nize smo pridobili s pomočjo *YouTube Data Tools* (https://tools.digitalmethods.net/netvizz/youtube/). Orodje je sprogramiral "Bernhard Rieder [...] an associate professor in New Media and Digital Culture at the University of Amsterdam and a researcher with the Digital Methods Initiative." (http://thepoliticsofsystems.net/about/).

Dva video posnetka na portalu *YouTube* sta povezana z znanima slovenskima politikoma, *Zoranom Jankovićem* in *Janezom Janšo*. Gledalci oddaje 24 ur so po ogledu oddali svoje pripombe. V tretjem video posnetku (file = 2) je novinarka spraševala ljudi, kaj delajo za božič. Gledalci so po ogledu oddali svoje pripombe.  

```{r}
gpath <-list.files(path = "data/youtube/",
                   pattern = "_comments.csv", 
                   full.names = TRUE)

comments <- map_dfr(gpath, read_csv, .id = "file") %>% 
  # dodelimo vsaki datoteki prepoznavno ime
  # namesto ifelse() uporabljamo case_when()
  mutate(file = case_when(
    file == "1" ~ "24ur_Jansa",
    file == "2" ~ "24ur_bozic",
    file == "3" ~ "24ur_Jankovic",
    TRUE ~ "other"
  ))

names(comments)
```


### Nezaželene besede

```{r}
stop_sl <- quanteda::stopwords(language = "sl", 
                               source = "stopwords-iso")

stop_sl <- c(quanteda::stopwords(language = "sl", 
                       source = "stopwords-iso"), 
             "\n", " ", "[\\d]+", "quot", "še", "^www.+", "^http.+", 
             "search_query")

stop_sl_tidy <- stop_sl %>% as_tibble() %>% rename(word = value)

# remove stopwords and punctuation and digits
stop_sl_collapsed <- paste0(paste0('\\b', stop_sl, '\\b', 
     collapse = "|"), '|[[:punct:]]+', "[\\d]+")

# another variant
stops <- paste0(stop_sl, collapse = "\\b|\\b")
```

Nekaj naključno izbranih pripomb gledalcev. 

```{r}
comments %>% slice_sample(n = 10)
```

### Uporaba udpipe

Najprej naložimo jezikovni model za slovenščino v pomnilnik računalnika. Prekopiramo programski odstavek iz prejšnjega poglavja. 

```{r message=FALSE, warning=FALSE}
#| eval: false

library(udpipe)
destfile = "slovenian-ssj-ud-2.5-191206.udpipe"

if(!file.exists(destfile)){
   jezikovni_model <- udpipe_download_model(language = "slovenian")
   udmodel_sl <- udpipe_load_model(jezikovni_model$file_model)
   } else {
  file_model = destfile
  udmodel_sl <- udpipe_load_model(file_model)
}

```

Prekopiramo tudi programski odstavek z lastnoročno izdelano programsko funkcijo za prilagajanje in dopolnjevanje tabele. 

```{r message=FALSE, warning=FALSE}
#| eval: false

tokenize_annotate = function(tbl){
  tbl %>% 
  unnest_tokens(word, token, drop = F) %>% 
  cbind_morphological(term = "feats",  
                      which = c("PronType","NumType","Poss","Reflex",
                                "Foreign","Abbr","Typo",
                                "Gender","Animacy","NounClass",
                                "Case","Number","Definite","Degree",
                                "VerbForm","Person","Tense","Mood",
                                "Aspect","Voice","Evident",
                                "Polarity","Polite","Clusivity")) %>% 
  mutate(txt = str_replace_all(sentence, "[:punct:]", "")) %>% 
  mutate(sentlen = quanteda::ntoken(txt)) %>% 
  mutate(syllables = nsyllable::nsyllable(txt)) %>% 
  mutate(types = quanteda::ntype(txt)) %>% 
  mutate(wordlen = syllables/sentlen) %>% 
  mutate(ttr = types/sentlen) %>% 
  select(-txt, -feats)
}

```

Naslednji trije programski odstavki (podobno kot v prejšnjem poglavju) poskrbijo za označevanje besednih oblik v prispevkih gledalcev YouTube video prispevka, dodajamo pa tudi stolpec za razlikovanje treh vzorcev. 

```{r message=FALSE, warning=FALSE}
#| eval: false
jansa <- comments %>% filter(file == "24ur_Jansa")
x = udpipe_annotate(udmodel_sl, x = jansa$text, trace = F)
jansa_ud = as.data.frame(x)

jansa_udpiped <- jansa_ud %>% 
  tokenize_annotate() %>% 
  mutate(file = "jansa") %>% 
  mutate(token_id = as.numeric(token_id),
         head_token_id = as.numeric(head_token_id))

```

```{r message=FALSE, warning=FALSE}
#| eval: false
bozic <- comments %>% filter(file == "24ur_bozic")
x = udpipe_annotate(udmodel_sl, x = bozic$text, trace = F)
bozic_ud = as.data.frame(x)

bozic_udpiped <- bozic_ud %>% 
  tokenize_annotate() %>% 
  mutate(file = "bozic") %>% 
  mutate(token_id = as.numeric(token_id),
         head_token_id = as.numeric(head_token_id))

```

```{r message=FALSE, warning=FALSE}
#| eval: false
janko <- comments %>% filter(file == "24ur_Jankovic")
x = udpipe_annotate(udmodel_sl, x = janko$text, trace = F)
janko_ud = as.data.frame(x)

janko_udpiped <- janko_ud %>% 
  tokenize_annotate() %>% 
  mutate(file = "jankovic") %>% 
  mutate(token_id = as.numeric(token_id),
         head_token_id = as.numeric(head_token_id))

```

Združujemo vzorce v eno tabelo. 

```{r}
#| eval: false
g_udpiped <- bind_rows(jansa_udpiped, bozic_udpiped, janko_udpiped)
dim(g_udpiped)
```
Shranimo za kasnejšo ponovno uporabo, in sicer v varčni obliki `rds`. Te datoteke odpira le `R`. 

```{r}
#| eval: false
saveRDS(g_udpiped, 
        "data/youtube_comments_sl_jansa-bozic-jankovic.rds")
```

Da prihranimo nekaj časa, prejšnjih pet odstavkov ni bilo izvedenih (#| `eval: false`). Zato datoteko v naslednjem odstavku naložimo v pomnilnik. 

```{r}
g_udpiped <- read_rds("data/youtube_comments_sl_jansa-bozic-jankovic.rds")
```


### Glagoli

Na hitro poglejmo, kateri glagoli v prispevkih gledalcev prevladujejo. 

V preglednici opazimo nekaj napak: besedila bi bilo treba sistematično pregledovati in popraviti najbolj moteče nepravilnosti kot npr. manjkajoč presledek za piko (npr. *tega.Zelo*) in očitne tipkovne napake gledalcev pri pisanju prispevkov (npr. *jepostavil*). 

```{r}
g_udpiped %>% 
  filter(upos == "VERB") %>% 
  select(token, lemma, upos, xpos) %>% 
  rmarkdown::paged_table()
```

Katere oblikoslovne oblike glagolov prevladujejo?

```{r}
g_udpiped %>% 
  filter(upos == "VERB") %>% 
  select(token, lemma, upos, xpos) %>% 
  count(xpos, sort = T) %>% 
  rmarkdown::paged_table()
```

Ali se pogostnost glagolov razlikuje glede na tematiko video prispevka, ki so si ga gledalci ogledali?

```{r}
verbs_in_comments <- g_udpiped %>%
  filter(upos == "VERB") %>% 
  filter(!str_detect(lemma, "^http")) %>% 
  filter(!str_detect(lemma, ";sti")) %>% 
  group_by(file) %>% 
  count(lemma, sort = T) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  pivot_wider(names_from = file, values_from = c(pct, n), values_fill = 0) %>% 
  arrange(-pct_jansa)

verbs_in_comments %>% rmarkdown::paged_table()

# write.csv2(verbs_in_comments, "data/youtube_sl_verbs_comments.csv")
```

Pripombe gledalcev k trem oddajam v obliki besednih oblakov:   

```{r}
#| out-width: "100%"
#| fig-width: 15
#| fig-height: 5

p0 <- g_udpiped %>%
  filter(upos == "VERB") %>% 
  filter(!str_detect(lemma, "^http")) %>% 
  filter(!str_detect(lemma, ";sti")) %>% 
  group_by(file) %>% 
  count(lemma, sort = T) %>% 
  mutate(pct = round(100*n/sum(n),2)) %>% 
  slice_head(n = 40) %>% 
  ggplot(aes(pct/100, label = lemma, size = log(pct), color = pct)) +
  geom_text_wordcloud() +
  theme_light() +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(x="") +
  facet_grid(~ file, scales = "free")

ggsave("pictures/youtube_comments_sl_verbs.png", dpi = 300,
       width = 10, height = 4)

p0
```

Glagol *biti* je seveda najpogostejši glagol v vseh treh skupinah, vendar s precej višjim odstotkom v prispevkih o božiču. V pripombah k oddajam o politikoma poleg glagola *biti* izstopa predvsem glagol *imeti*. Mogoče se bo ta (pogostnostna) razlika pokazala tudi pri kolokacijah, pri katerih so sestavni del glagolske oblike. 

Podoben pregled značilnih slovarskih enot bi lahko naredili tudi npr. za samostalnike in pridevnike. 

Preštejmo še druge besede.  

```{r}
# count words and remove numbers (if still present)
word_count <- g_udpiped %>% 
  count(word, sort = T, name = "Freq_word") %>% 
  filter(!str_detect(word, "[\\d]+"))

# Number of words
N <- nrow(word_count)
```

```{r}
words_video <- words %>% 
  group_by(file) %>% 
  # anti_join(stop_sl_tidy, by = "word") %>% # 6435 words
  # filter(!str_detect(word, stop_sl_collapsed)) %>% # 6294 words
  filter(!str_detect(word, stops)) %>% # 5278 words
  count(word, sort = T) %>% 
  drop_na() %>% 
  group_by(file) %>% 
  mutate(pct = round(100*n/sum(n), 3))

dim(words_video)
```

```{r}
words_video %>% 
  pivot_wider(names_from = file, values_from = n,
              values_fill = 0) %>% 
  ungroup()
```

```{r out.width="100%", fig.width=10, fig.height=5}
library(ggwordcloud)
p1 <- words_video %>% 
  slice_head(n = 50) %>% 
  mutate(angle = 45 * sample(-2:2, n(), replace = TRUE, 
                             prob = c(1, 1, 4, 1, 1))) %>% 
  ggplot(aes(label = word, size = pct, color = n), angle = angle) +
  geom_text_wordcloud(shape = "circle", rm_outside = TRUE) +
  scale_size_area(max_size = 20) +
  # scale_radius(range = c(0, 16), limits = c(0, NA)) +
  facet_wrap(~ file, scales = "free")

ggsave("pictures/youtube_sl_words_per_video.png", dpi = 300, 
       width = 10, height = 5)

p1
```

